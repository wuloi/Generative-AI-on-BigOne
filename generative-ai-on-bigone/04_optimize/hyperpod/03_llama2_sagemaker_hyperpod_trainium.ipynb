{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7ec338-4468-4f59-ba9a-16ac395d9945",
   "metadata": {},
   "source": [
    "# Training Llama2 with Amazon SageMaker HyperPod with NeuronX, Nemo, and Megatron on Trainium (Trn1)\n",
    "\n",
    "_Derived from this GitHub repo: https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/8.neuronx-nemo-megatron_\n",
    "\n",
    "[AWS Neuron Reference for NeMo Megatron](https://github.com/aws-neuron/neuronx-nemo-megatron/tree/main)(`neuronx-nemo-megatron`) is a modified versions of open-source packages [Nemo](https://github.com/NVIDIA/NeMo) and [Apex](https://github.com/NVIDIA/apex) that have been adapted for use with [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/) and [Amazon EC2 Trn1 instance](https://aws.amazon.com/ec2/instance-types/trn1/). This test case describes how to run Llama2 training on Slurm with Trn1 instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f4b66-b5ad-46b4-a184-ad94726c1a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This guide assumes that you have the following:\n",
    "* A functional Slurm cluster on AWS. We also assume that Ubuntu AMI is used.\n",
    "* Neuron SDK is installed on the cluster (see [AWS Neuron SDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx) for the steps).\n",
    "* An FSx for Lustre filesystem mounted on `/fsx`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd579e5-05ca-47bf-9202-47341ccbe94e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup Environment\n",
    "First of all, you need to have a Python virtual environment for `torch-neuronx` under `APPS_PATH`.\n",
    "\n",
    "```bash\n",
    "bash 1.setup-venv.sh ${APPS_PATH} # The argument specifies APPS_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3b38c-d0b9-49f2-b828-29a206ed5777",
   "metadata": {},
   "source": [
    "## 2. `neuronx-nemo-megatron` library need to be installed (and initialized) in the environment.\n",
    "\n",
    "\n",
    "```bash\n",
    "bash 2.setup-neuronx-nemo-megatron.sh ${APPS_PATH} #\n",
    "```\n",
    "You will see the following ERROR line during the script execution. This is safe to ignore.\n",
    "\n",
    "```console\n",
    "+ python3 -c 'from nemo.collections.nlp.data.language_modeling.megatron.dataset_utils import compile_helper; compile_helper()'\n",
    "2023-Nov-18 09:17:45.728072 175272:175272 ERROR  TDRV:tdrv_get_dev_info                       No neuron device available\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e428098-b662-48d8-b8bb-306235b4e26f",
   "metadata": {},
   "source": [
    "## 1. Prepare Llama2 model\n",
    "\n",
    "We recommend that you setup a Slurm cluster using the template in the architectures directory. Before creating the Slurm cluster, you need to setup the following environment variables:\n",
    "\n",
    "```bash\n",
    "export APPS_PATH=/fsx\n",
    "export FSX_PATH=/fsx\n",
    "export MODEL_PATH=/fsx\n",
    "export DATA_PATH=$FSX_PATH/data/books\n",
    "export TEST_CASE_PATH=${APPS_PATH}/awsome-distributed-training/3.test_cases/8.neuronx-nemo-megatron  # where you copy the test case or set to your test case path\n",
    "```\n",
    "\n",
    "This test case requires Llama2 model, which governed by the Meta license and must be downloaded and converted to the standard [Hugging Face](https://huggingface.co/) format prior to running this sample.\n",
    "You can submit access request from [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/), we need \"Llama 2 & Llama Chat\" to be checked. Use the [download.sh](https://github.com/facebookresearch/llama/blob/main/download.sh) in the official repository. You will be asked to input an URL from the email you recieve from meta.  \n",
    "\n",
    "We will assume that you had placed the model and tokenizer as follows on cluster:\n",
    "\n",
    "```\n",
    "${MODEL_PATH}/Llama2-meta/\n",
    "├── 7B/\n",
    "│   ├── checklist.chk\n",
    "│   ├── consolidated.00.pth\n",
    "│   └── params.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_checklist.chk\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190313a-b7f9-4b6e-b39d-79779dc6a5b3",
   "metadata": {},
   "source": [
    "## 3. Convert weights to HuggingFace format\n",
    "To convert the model to the standard Hugging Face format, the following script in transformers can be called with the following (example) command:\n",
    "\n",
    "```\n",
    "sbatch 3.convert-weight.sbatch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e893096-b3b3-4ea1-83d9-4540e30aeb8c",
   "metadata": {},
   "source": [
    "You can check progress of with `tail` command.\n",
    "\n",
    "```\n",
    "$ tail -f slurm-3.convert-weight.sbatch-xxx.out \n",
    "```\n",
    "\n",
    "```console\n",
    "Fetching all parameters from the checkpoint at /fsx/Llama2-meta/7B.\n",
    "Loading the checkpoint in a Llama model.\n",
    "Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.65it/s]\n",
    "...\n",
    "```\n",
    "\n",
    "Once the job completed, you will have the Llama-2-7b model weights and tokenizer in a huggingface format under a directory called `Llama2-7b-hf` with the following format:\n",
    "\n",
    "```console\n",
    "${DATAPATH}/Llama2-7b-hf/\n",
    "├── config.json\n",
    "├── generation_config.json\n",
    "├── pytorch_model-00001-of-00002.bin\n",
    "├── pytorch_model-00002-of-00002.bin\n",
    "├── pytorch_model.bin.index.json\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0dc8f4-6b55-4e66-acc9-a18b2a084e56",
   "metadata": {},
   "source": [
    "## 4. Download and Tokenize dataset\n",
    "This tutorial makes use of a [Red pyjama dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T). The dataset can be downloaded to your cluster by running the following commands on the head node:\n",
    "\n",
    "```bash\n",
    "mkdir -p ${DATA_PATH} \n",
    "wget https://data.together.xyz/redpajama-data-1T/v1.0.0/book/book.jsonl -O ${DATA_PATH}/book.jsonl # Note: Dataset download is 50G and will take approximately 3-4 hours to download. You can also use https://aria2.github.io/ for faster download\n",
    "# or\n",
    "# wget https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample/resolve/main/book_sample.jsonl -O ${DATA_PATH}/book.jsonl # Smaller sample dataset for quick testing\n",
    "```\n",
    "\n",
    "Once you have the Tokenizer and the dataset. You can tokenize the dataset following the below command: \n",
    "```bash\n",
    "sbatch 4.tokenize.sbatch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb2ce9-e7da-4624-a298-c56b9003158d",
   "metadata": {},
   "source": [
    "Post tokenizing the dataset, you will have a path to the tokenizer and the dataset which will be used for pretraining. \n",
    "\n",
    "## Llama2 training configurations\n",
    "We tested with the following model sizes: 7B\n",
    "### Llama2 7B\n",
    "\n",
    "- Model configuration\n",
    "    - Attention heads: 32\n",
    "    - Layers: 32\n",
    "    - Sequence length: 4096\n",
    "    - Hidden size: 4096\n",
    "    - Hidden FFN size: 11008\n",
    "    - Microbatch size: 1\n",
    "    - Global batch size: 256\n",
    "\n",
    "- Distributed training configuration\n",
    "    - Number of nodes: 4\n",
    "    - Tensor parallel degree: 8\n",
    "    - Pipeline parallel degree: 1\n",
    "    - Data parallel degree: 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002981a-4275-448e-920c-220900140e0c",
   "metadata": {},
   "source": [
    "## 5. Pre-compile the model\n",
    "By default, PyTorch Neuron uses a just in time (JIT) compilation flow that sequentially compiles all of the neural network compute graphs as they are encountered during a training job. The compiled graphs are cached in a local compiler cache so that subsequent training jobs can leverage the compiled graphs and avoid compilation (so long as the graph signatures and Neuron version have not changed).\n",
    "\n",
    "An alternative to the JIT flow is to use the included [neuron_parallel_compile](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html?highlight=neuron_parallel_compile) command to perform ahead of time (AOT) compilation. In the AOT compilation flow, the compute graphs are first identified and extracted during a short simulated training run, and the extracted graphs are then compiled and cached using parallel compilation, which is considerably faster than the JIT flow.\n",
    "\n",
    "Before starting the compilation you need to update your path to the dataset and tokenizer in the llama_7b script as below : \n",
    "\n",
    "```bash\n",
    "cd ${APPS_PATH}/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling\n",
    "vi test_llama.sh\n",
    "```\n",
    "\n",
    "Update the below lines\n",
    "\n",
    "```bash\n",
    ": ${TOKENIZER_PATH=$HOME/llamav2_weights/7b-hf}\n",
    ": ${DATASET_PATH=$HOME/examples_datasets/llama_7b/book.jsonl-processed_text_document}\n",
    "```\n",
    "\n",
    "to\n",
    "```bash\n",
    ": ${TOKENIZER_PATH=${MODEL_PATH}/Llama2-7b-hf}\n",
    ": ${DATASET_PATH=${DATA_PATH}/book-tokenized_text_document}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03638422-afe1-45f9-a731-398d9ff27e59",
   "metadata": {},
   "source": [
    "Then, run the following command to launch an AOT pre-compilation job on your ParallelCluster:\n",
    "\n",
    "```bash\n",
    "bash 5.precompile-model.sh\n",
    "```\n",
    "\n",
    "Once you have launched the precompilation job, run the `squeue` command to view the SLURM job queue on your cluster. If you have not recently run a job on your cluster, it may take 4-5 minutes for the requested trn1.32xlarge nodes to be launched and initialized. Once the job is running, `squeue` should show output similar to the following:\n",
    "\n",
    "```console\n",
    "    JOBID  PARTITION  NAME           USER    ST  TIME  NODES NODELIST(REASON)\n",
    "    10     compute1   compile.slurm  ubuntu  R   5:11  4     compute1-dy-queue1-i1-[1-4]\n",
    "```\n",
    "\n",
    "You can view the output of the precompilation job by examining the file named `slurm-compile.slurm-ZZ.out` where ZZ represents the JOBID of your job in the `squeue` output, above. Ex:\n",
    "```\n",
    "tail -f slurm-compile.slurm-10.out\n",
    "```\n",
    "\n",
    "Once the precompilation job is complete, you should see a message similar to the following in the logs:\n",
    "\n",
    "```console\n",
    "2023-06-11 23:04:08.000738: INFO ||PARALLEL_COMPILE||: Total graphs: 22\n",
    "2023-06-11 23:04:08.000738: INFO ||PARALLEL_COMPILE||: Total successful compilations: 22\n",
    "2023-06-11 23:04:08.000738: INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n",
    "```\n",
    "\n",
    "At this point, you can press `CTRL-C` to exit the tail command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c350131-e69d-4a93-86a2-fc4f6b867e06",
   "metadata": {},
   "source": [
    "## 6. Launch a pretraining job\n",
    "\n",
    "Submit the training job\n",
    "\n",
    "```\n",
    "bash 6.pretrain-model.sh\n",
    "```\n",
    "\n",
    "\n",
    "As outlined above, you can again use the `squeue` command to view the job queue. Once you see that your pretraining job is running, you can view the output of the training job by examining the file named `slurm-run.slurm-ZZ.out` where ZZ represents the JOBID of your job:\n",
    "\n",
    "```bash\n",
    "tail -f slurm-run.slurm-11.out\n",
    "```\n",
    "\n",
    "Once the model is loaded onto the Trainium accelerators and training has commenced, you will begin to see output indicating the job progress:\n",
    "\n",
    "```console\n",
    "Epoch 0:  22%|██▏       | 4499/20101 [22:26:14<77:48:37, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\n",
    "Epoch 0:  22%|██▏       | 4500/20101 [22:26:32<77:48:18, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\n",
    "Epoch 0:  22%|██▏       | 4500/20101 [22:26:32<77:48:18, 17.95s/it, loss=2.44, v_num=5563, reduced_train_loss=2.450, gradient_norm=0.120, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.50]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaee5a9-02e2-4f3e-8598-982b7b96f915",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "* [A] Keita Watanabe - mlkeita@\n",
    "* [R] Verdi March - marcverd@\n",
    "* [R] Brad Doran \n",
    "* [R] Justin Pirtle \n",
    "* [R] Pierre-Yves Aquilanti - pierreya@\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
