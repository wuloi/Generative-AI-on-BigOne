{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) is the next version of the [LLaMA](https://arxiv.org/abs/2302.13971). Compared to the V1 model, it is trained on more data - 2T tokens and supports context length window upto 4K tokens. Learn more about LLaMa 2 in the [\"\"]() blog post.\n",
    "\n",
    "LoRA is an efficient finetuning technique that attaches small “Low-Rank Adapters” which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, LoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with LoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- IA3: [Infused Adapter by Inhibiting and Amplifying Inner Activations](https://arxiv.org/abs/2205.05638)\n",
    "\n",
    "\n",
    "\n",
    "### Access LLaMA 2\n",
    "\n",
    "Before we can start training we have to make sure that we accepted the license of [llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at: \n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [LLaMa 13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)\n",
    "* [LLaMa 70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::079002598131:role/service-role/AmazonSageMaker-ExecutionRole-20220804T150518\n",
      "sagemaker bucket: sagemaker-us-east-1-079002598131\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "we will use the [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `samsum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'What car manufacturers were associated with American Muscle cars in the 1960s and 1970s?', 'context': '', 'response': 'Chevrolet, Ford, Dodge', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "How would you describe the taste of coffee?\n",
      "\n",
      "### Answer\n",
      "A hot coffee has an essence of oats, earth, and a bitter chocolate or cacao. Commonly served with some form dairy to cut through the natural bitter taste, and to add additional sweetness. When iced, coffee can transform into a refreshing beverage that has more of a spring morning tone to the aftertaste.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights, gated\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # not gated, TODO: Try 13b\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-a9e8aa13e41a7fc8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-9286c08bce910be1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75f16e9e4c95140b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Identify which car manufacturer is German or American: Audi, Buick\n",
      "\n",
      "### Answer\n",
      "Audi is German, Buick is American</s>\n",
      "Total number of samples: 1591\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-079002598131/processed/llama/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 13B \n",
    "\n",
    "We prepared a [train.py](./scripts/train.py)\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "#  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "#  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p4d.24xlarge',   # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-lora-2023-09-01-01-15-21-2023-09-01-01-15-22-117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-01 01:15:22 Starting - Starting the training job...\n",
      "2023-09-01 01:15:37 Pending - Training job waiting for capacity.........\n",
      "2023-09-01 01:17:04 Pending - Preparing the instances for training...........................\n",
      "2023-09-01 01:21:47 Downloading - Downloading input data\n",
      "2023-09-01 01:21:47 Training - Downloading the training image...........................\n",
      "2023-09-01 01:26:08 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:24,877 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:24,932 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:24,940 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:24,941 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:26,371 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 77.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 60.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 81.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, transformers, accelerate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 safetensors-0.3.3 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:33,943 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:33,943 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:34,021 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:34,084 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:34,147 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:34,156 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 3,\n",
      "        \"lr\": 0.0002,\n",
      "        \"model_id\": \"NousResearch/Llama-2-7b-hf\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-lora-2023-09-01-01-15-21-2023-09-01-01-15-22-117\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-079002598131/huggingface-lora-2023-09-01-01-15-21-2023-09-01-01-15-22-117/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":3,\"lr\":0.0002,\"model_id\":\"NousResearch/Llama-2-7b-hf\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-079002598131/huggingface-lora-2023-09-01-01-15-21-2023-09-01-01-15-22-117/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":3,\"lr\":0.0002,\"model_id\":\"NousResearch/Llama-2-7b-hf\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-lora-2023-09-01-01-15-21-2023-09-01-01-15-22-117\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-079002598131/huggingface-lora-2023-09-01-01-15-21-2023-09-01-01-15-22-117/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"3\",\"--lr\",\"0.0002\",\"--model_id\",\"NousResearch/Llama-2-7b-hf\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=NousResearch/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 3 --lr 0.0002 --model_id NousResearch/Llama-2-7b-hf --per_device_train_batch_size 2\u001b[0m\n",
      "\u001b[34m2023-09-01 01:27:34,183 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 583/583 [00:00<00:00, 5.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 159MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:27, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:33, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:31, 317MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 178M/9.98G [00:00<00:26, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 220M/9.98G [00:00<00:25, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 262M/9.98G [00:00<00:25, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 304M/9.98G [00:00<00:26, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 346M/9.98G [00:00<00:27, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 388M/9.98G [00:01<00:29, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 430M/9.98G [00:01<00:30, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 472M/9.98G [00:01<00:29, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 514M/9.98G [00:01<00:30, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 556M/9.98G [00:01<00:29, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 598M/9.98G [00:01<00:27, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▋         | 640M/9.98G [00:02<00:35, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 682M/9.98G [00:02<00:33, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 724M/9.98G [00:02<00:30, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 776M/9.98G [00:02<00:27, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 818M/9.98G [00:02<00:26, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 860M/9.98G [00:02<00:28, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 902M/9.98G [00:02<00:28, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 944M/9.98G [00:02<00:29, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 975M/9.98G [00:03<00:30, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 1.01G/9.98G [00:03<00:31, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:03<00:32, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.07G/9.98G [00:03<00:31, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.11G/9.98G [00:03<00:29, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█▏        | 1.14G/9.98G [00:03<00:29, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.18G/9.98G [00:03<00:27, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.23G/9.98G [00:03<00:27, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.27G/9.98G [00:03<00:25, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.31G/9.98G [00:04<00:26, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▎        | 1.35G/9.98G [00:04<00:27, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.39G/9.98G [00:04<00:26, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.44G/9.98G [00:04<00:28, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:04<00:32, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.50G/9.98G [00:04<00:31, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.53G/9.98G [00:04<00:32, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:05<00:29, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.60G/9.98G [00:05<00:28, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▋        | 1.64G/9.98G [00:05<00:34, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.67G/9.98G [00:05<00:34, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.70G/9.98G [00:05<00:33, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.74G/9.98G [00:05<00:30, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.77G/9.98G [00:05<00:30, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.80G/9.98G [00:05<00:30, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:06<00:29, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.88G/9.98G [00:06<00:29, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.91G/9.98G [00:06<00:29, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.95G/9.98G [00:06<00:27, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:06<00:26, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.02G/9.98G [00:06<00:28, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.07G/9.98G [00:06<00:26, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:06<00:28, 278MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██▏       | 2.13G/9.98G [00:07<00:27, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:07<00:27, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.19G/9.98G [00:07<00:29, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.22G/9.98G [00:07<00:29, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:07<00:29, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.32G/9.98G [00:07<00:24, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:07<00:26, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.39G/9.98G [00:08<00:26, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.42G/9.98G [00:08<00:27, 274MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.45G/9.98G [00:08<00:27, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.49G/9.98G [00:08<00:29, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:08<00:28, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.55G/9.98G [00:08<00:28, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.59G/9.98G [00:08<00:25, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:08<00:25, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.65G/9.98G [00:08<00:26, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.69G/9.98G [00:09<00:24, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:09<00:23, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:09<00:23, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.80G/9.98G [00:09<00:28, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.84G/9.98G [00:09<00:25, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.88G/9.98G [00:09<00:24, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.92G/9.98G [00:09<00:24, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.95G/9.98G [00:10<00:24, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:10<00:31, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.01G/9.98G [00:10<00:32, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.05G/9.98G [00:10<00:27, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.08G/9.98G [00:10<00:26, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.11G/9.98G [00:10<00:24, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.16G/9.98G [00:10<00:23, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.20G/9.98G [00:10<00:22, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.24G/9.98G [00:11<00:20, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.28G/9.98G [00:11<00:21, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.32G/9.98G [00:11<00:22, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 3.37G/9.98G [00:11<00:21, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.41G/9.98G [00:11<00:21, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.45G/9.98G [00:11<00:20, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.49G/9.98G [00:11<00:19, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 3.53G/9.98G [00:11<00:18, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.58G/9.98G [00:12<00:18, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▋      | 3.62G/9.98G [00:12<00:18, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.66G/9.98G [00:12<00:20, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.70G/9.98G [00:12<00:19, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.74G/9.98G [00:12<00:17, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.79G/9.98G [00:12<00:18, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:12<00:17, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:13<00:17, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:13<00:16, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.96G/9.98G [00:13<00:17, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.01G/9.98G [00:13<00:17, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:13<00:18, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:13<00:17, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████▏     | 4.13G/9.98G [00:13<00:16, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.17G/9.98G [00:13<00:16, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:13<00:16, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.26G/9.98G [00:14<00:16, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.30G/9.98G [00:14<00:15, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▎     | 4.34G/9.98G [00:14<00:15, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.38G/9.98G [00:14<00:16, 336MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:14<00:15, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.47G/9.98G [00:14<00:16, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:14<00:15, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:14<00:15, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:15<00:15, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:15<00:14, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:15<00:15, 349MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.72G/9.98G [00:15<00:15, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.76G/9.98G [00:15<00:14, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.81G/9.98G [00:15<00:13, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▊     | 4.85G/9.98G [00:15<00:14, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.90G/9.98G [00:15<00:15, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.94G/9.98G [00:16<00:15, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:16<00:14, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:16<00:14, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.06G/9.98G [00:16<00:14, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████▏    | 5.12G/9.98G [00:16<00:13, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.17G/9.98G [00:16<00:12, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:16<00:12, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.25G/9.98G [00:16<00:13, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:17<00:14, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:17<00:15, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.38G/9.98G [00:17<00:14, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.42G/9.98G [00:17<00:14, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 5.46G/9.98G [00:17<00:14, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.51G/9.98G [00:17<00:13, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.55G/9.98G [00:17<00:13, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:18<00:14, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▋    | 5.63G/9.98G [00:18<00:13, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.68G/9.98G [00:18<00:12, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.73G/9.98G [00:18<00:11, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.77G/9.98G [00:18<00:11, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.81G/9.98G [00:18<00:11, 358MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▊    | 5.85G/9.98G [00:18<00:11, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.89G/9.98G [00:18<00:11, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.93G/9.98G [00:18<00:11, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|█████▉    | 5.98G/9.98G [00:19<00:10, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.02G/9.98G [00:19<00:11, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.06G/9.98G [00:19<00:10, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:19<00:10, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.14G/9.98G [00:19<00:12, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.19G/9.98G [00:19<00:13, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.23G/9.98G [00:19<00:13, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:20<00:12, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.31G/9.98G [00:20<00:11, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▎   | 6.35G/9.98G [00:20<00:12, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.39G/9.98G [00:20<00:16, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.42G/9.98G [00:20<00:15, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.46G/9.98G [00:20<00:13, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.49G/9.98G [00:20<00:13, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.53G/9.98G [00:21<00:11, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.57G/9.98G [00:21<00:10, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▋   | 6.62G/9.98G [00:21<00:10, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.66G/9.98G [00:21<00:11, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.69G/9.98G [00:21<00:11, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.73G/9.98G [00:21<00:10, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:21<00:10, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.82G/9.98G [00:21<00:10, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:22<00:09, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.90G/9.98G [00:22<00:09, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.94G/9.98G [00:22<00:10, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.98G/9.98G [00:22<00:10, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|███████   | 7.01G/9.98G [00:22<00:10, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.05G/9.98G [00:22<00:11, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.08G/9.98G [00:22<00:11, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:23<00:10, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.15G/9.98G [00:23<00:09, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.19G/9.98G [00:23<00:08, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.24G/9.98G [00:23<00:09, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.28G/9.98G [00:23<00:08, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.32G/9.98G [00:23<00:08, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▎  | 7.35G/9.98G [00:23<00:08, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:23<00:09, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.41G/9.98G [00:24<00:11, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:24<00:11, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:24<00:11, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.51G/9.98G [00:24<00:11, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.54G/9.98G [00:24<00:11, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.57G/9.98G [00:24<00:10, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.60G/9.98G [00:25<00:10, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.64G/9.98G [00:25<00:09, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.69G/9.98G [00:25<00:08, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.73G/9.98G [00:25<00:07, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.77G/9.98G [00:25<00:07, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.80G/9.98G [00:25<00:07, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▊  | 7.83G/9.98G [00:25<00:07, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.87G/9.98G [00:25<00:07, 299MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.91G/9.98G [00:25<00:07, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.95G/9.98G [00:26<00:06, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.98G/9.98G [00:26<00:06, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|████████  | 8.01G/9.98G [00:26<00:07, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.04G/9.98G [00:26<00:06, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:26<00:06, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 8.12G/9.98G [00:26<00:06, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.15G/9.98G [00:26<00:06, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.19G/9.98G [00:26<00:06, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:27<00:06, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [00:27<00:06, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:27<00:06, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:27<00:07, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▎ | 8.35G/9.98G [00:27<00:07, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.38G/9.98G [00:27<00:06, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.41G/9.98G [00:27<00:06, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [00:28<00:06, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.47G/9.98G [00:28<00:05, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 8.51G/9.98G [00:28<00:05, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [00:28<00:05, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.59G/9.98G [00:28<00:04, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▋ | 8.62G/9.98G [00:28<00:04, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.65G/9.98G [00:28<00:06, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.68G/9.98G [00:29<00:06, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.72G/9.98G [00:29<00:05, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [00:29<00:05, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.79G/9.98G [00:29<00:05, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [00:29<00:05, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:29<00:04, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.89G/9.98G [00:29<00:04, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.93G/9.98G [00:30<00:03, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.98G/9.98G [00:30<00:03, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [00:30<00:03, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:30<00:03, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [00:30<00:02, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████▏| 9.12G/9.98G [00:30<00:03, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:30<00:03, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.19G/9.98G [00:30<00:03, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.22G/9.98G [00:31<00:02, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.25G/9.98G [00:31<00:03, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.28G/9.98G [00:31<00:03, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.31G/9.98G [00:31<00:03, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:31<00:02, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.37G/9.98G [00:31<00:02, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.41G/9.98G [00:31<00:02, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 9.44G/9.98G [00:32<00:02, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [00:32<00:02, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 9.50G/9.98G [00:32<00:02, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.53G/9.98G [00:32<00:01, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.56G/9.98G [00:32<00:01, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:32<00:01, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:32<00:01, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.67G/9.98G [00:33<00:01, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.70G/9.98G [00:33<00:01, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [00:33<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:33<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.81G/9.98G [00:33<00:00, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.86G/9.98G [00:33<00:00, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:33<00:00, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.93G/9.98G [00:33<00:00, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:34<00:00, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:34<00:00, 293MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:34<00:34, 34.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 31.5M/3.50G [00:00<00:12, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 73.4M/3.50G [00:00<00:10, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 115M/3.50G [00:00<00:09, 357MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 168M/3.50G [00:00<00:08, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 210M/3.50G [00:00<00:08, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 262M/3.50G [00:00<00:07, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 315M/3.50G [00:00<00:07, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 367M/3.50G [00:00<00:07, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 409M/3.50G [00:01<00:11, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 451M/3.50G [00:01<00:10, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 493M/3.50G [00:01<00:10, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 524M/3.50G [00:01<00:11, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 556M/3.50G [00:01<00:10, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 598M/3.50G [00:01<00:09, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:01<00:10, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 682M/3.50G [00:02<00:08, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 724M/3.50G [00:02<00:08, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 765M/3.50G [00:02<00:08, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 807M/3.50G [00:02<00:07, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 849M/3.50G [00:02<00:07, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 891M/3.50G [00:02<00:08, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:02<00:07, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 975M/3.50G [00:02<00:07, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 1.02G/3.50G [00:03<00:07, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 1.06G/3.50G [00:03<00:07, 331MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███▏      | 1.10G/3.50G [00:03<00:07, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 1.14G/3.50G [00:03<00:07, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 1.18G/3.50G [00:03<00:06, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 1.24G/3.50G [00:03<00:06, 358MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 1.28G/3.50G [00:03<00:06, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 1.32G/3.50G [00:04<00:06, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 1.36G/3.50G [00:04<00:05, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 1.41G/3.50G [00:04<00:05, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████▏     | 1.45G/3.50G [00:04<00:05, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 1.50G/3.50G [00:04<00:04, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 1.54G/3.50G [00:04<00:05, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:04<00:05, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 1.64G/3.50G [00:04<00:04, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 1.68G/3.50G [00:04<00:04, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 1.72G/3.50G [00:05<00:04, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 1.76G/3.50G [00:05<00:04, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 1.80G/3.50G [00:05<00:05, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 1.85G/3.50G [00:05<00:05, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:05<00:05, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 1.93G/3.50G [00:05<00:04, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 1.98G/3.50G [00:05<00:04, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 2.02G/3.50G [00:05<00:04, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:06<00:04, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 2.11G/3.50G [00:06<00:04, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████▏   | 2.15G/3.50G [00:06<00:04, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 2.19G/3.50G [00:06<00:04, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:06<00:04, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 2.28G/3.50G [00:06<00:03, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 2.32G/3.50G [00:06<00:03, 330MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 2.36G/3.50G [00:07<00:03, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 2.40G/3.50G [00:07<00:03, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 2.44G/3.50G [00:07<00:02, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:07<00:02, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:07<00:02, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▎  | 2.58G/3.50G [00:07<00:02, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:07<00:02, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▋  | 2.67G/3.50G [00:07<00:02, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 2.72G/3.50G [00:07<00:02, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 2.76G/3.50G [00:08<00:02, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 2.80G/3.50G [00:08<00:02, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:08<00:01, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:08<00:01, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:08<00:01, 347MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:08<00:01, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:08<00:01, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 3.05G/3.50G [00:09<00:01, 304MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 3.09G/3.50G [00:09<00:01, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:09<00:00, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:09<00:00, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:09<00:00, 337MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:09<00:00, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 3.31G/3.50G [00:09<00:00, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:09<00:00, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:10<00:00, 315MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:10<00:00, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 3.48G/3.50G [00:10<00:00, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:10<00:00, 339MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:44<00:00, 20.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:44<00:00, 22.24s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 179/179 [00:00<00:00, 1.90MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/2388 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/2388 [00:03<2:04:42,  3.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/2388 [00:05<1:42:12,  2.57s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/2388 [00:07<1:35:24,  2.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/2388 [00:09<1:32:10,  2.32s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/2388 [00:11<1:30:22,  2.28s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/2388 [00:14<1:29:17,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/2388 [00:16<1:28:35,  2.23s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/2388 [00:18<1:28:08,  2.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/2388 [00:20<1:27:48,  2.21s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/2388 [00:22<1:27:33,  2.21s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3035, 'learning_rate': 0.00019916247906197655, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m0%|          | 10/2388 [00:22<1:27:33,  2.21s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 11/2388 [00:25<1:27:22,  2.21s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 12/2388 [00:27<1:27:14,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 13/2388 [00:29<1:27:08,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 14/2388 [00:31<1:27:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 15/2388 [00:33<1:26:59,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 16/2388 [00:36<1:26:55,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 17/2388 [00:38<1:26:52,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 18/2388 [00:40<1:26:49,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 19/2388 [00:42<1:26:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 20/2388 [00:44<1:26:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 4.71, 'learning_rate': 0.0001983249581239531, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m1%|          | 20/2388 [00:44<1:26:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 21/2388 [00:47<1:26:43,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 22/2388 [00:49<1:26:38,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 23/2388 [00:51<1:26:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 24/2388 [00:53<1:26:34,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 25/2388 [00:55<1:26:32,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 26/2388 [00:58<1:26:28,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 27/2388 [01:00<1:26:25,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 28/2388 [01:02<1:26:22,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 29/2388 [01:04<1:26:20,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2388 [01:06<1:26:18,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 8.9335, 'learning_rate': 0.00019748743718592964, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2388 [01:06<1:26:18,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 31/2388 [01:09<1:26:16,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 32/2388 [01:11<1:26:14,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 33/2388 [01:13<1:26:12,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 34/2388 [01:15<1:26:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 35/2388 [01:17<1:26:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 36/2388 [01:20<1:26:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 37/2388 [01:22<1:26:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 38/2388 [01:24<1:26:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 39/2388 [01:26<1:25:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2388 [01:28<1:25:55,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 8.6527, 'learning_rate': 0.0001966499162479062, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2388 [01:28<1:25:55,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 41/2388 [01:30<1:25:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 42/2388 [01:33<1:25:52,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 43/2388 [01:35<1:25:49,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 44/2388 [01:37<1:25:47,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 45/2388 [01:39<1:25:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 46/2388 [01:41<1:25:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 47/2388 [01:44<1:25:40,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 48/2388 [01:46<1:25:38,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 49/2388 [01:48<1:25:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2388 [01:50<1:25:34,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 9.1475, 'learning_rate': 0.00019581239530988274, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2388 [01:50<1:25:34,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 51/2388 [01:52<1:25:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 52/2388 [01:55<1:25:31,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 53/2388 [01:57<1:25:28,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 54/2388 [01:59<1:25:25,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 55/2388 [02:01<1:25:22,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 56/2388 [02:03<1:25:19,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 57/2388 [02:06<1:25:17,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 58/2388 [02:08<1:25:15,  2.20s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 59/2388 [02:10<1:25:13,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2388 [02:12<1:25:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.8626, 'learning_rate': 0.0001949748743718593, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2388 [02:12<1:25:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 61/2388 [02:14<1:25:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 62/2388 [02:17<1:25:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 63/2388 [02:19<1:25:06,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 64/2388 [02:21<1:25:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 65/2388 [02:23<1:25:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 66/2388 [02:25<1:24:59,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 67/2388 [02:28<1:24:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 68/2388 [02:30<1:24:56,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 69/2388 [02:32<1:24:53,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2388 [02:34<1:24:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.6718, 'learning_rate': 0.00019413735343383584, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2388 [02:34<1:24:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 71/2388 [02:36<1:24:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 72/2388 [02:39<1:24:46,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 73/2388 [02:41<1:24:43,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 74/2388 [02:43<1:24:41,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 75/2388 [02:45<1:24:37,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 76/2388 [02:47<1:24:34,  2.19s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 77/2388 [02:50<1:24:31,  2.19s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 78/2388 [02:52<1:24:31,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 79/2388 [02:54<1:24:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2388 [02:56<1:24:29,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.5304, 'learning_rate': 0.0001932998324958124, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2388 [02:56<1:24:29,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 81/2388 [02:58<1:24:26,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 82/2388 [03:01<1:24:24,  2.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 83/2388 [03:03<1:24:21,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 84/2388 [03:05<1:24:19,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 85/2388 [03:07<1:24:16,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 86/2388 [03:09<1:24:13,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 87/2388 [03:12<1:24:12,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 88/2388 [03:14<1:24:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 89/2388 [03:16<1:24:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2388 [03:18<1:24:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.3042, 'learning_rate': 0.00019246231155778894, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2388 [03:18<1:24:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 91/2388 [03:20<1:24:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 92/2388 [03:22<1:24:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 93/2388 [03:25<1:23:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 94/2388 [03:27<1:23:56,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 95/2388 [03:29<1:23:53,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 96/2388 [03:31<1:23:51,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 97/2388 [03:33<1:23:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 98/2388 [03:36<1:23:47,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 99/2388 [03:38<1:23:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2388 [03:40<1:23:44,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2977, 'learning_rate': 0.0001916247906197655, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2388 [03:40<1:23:44,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 101/2388 [03:42<1:23:41,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 102/2388 [03:44<1:23:38,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 103/2388 [03:47<1:23:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 104/2388 [03:49<1:23:34,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 105/2388 [03:51<1:23:32,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 106/2388 [03:53<1:23:29,  2.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 107/2388 [03:55<1:23:27,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 108/2388 [03:58<1:23:25,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 109/2388 [04:00<1:23:23,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2388 [04:02<1:23:22,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.1958, 'learning_rate': 0.00019078726968174204, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2388 [04:02<1:23:22,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 111/2388 [04:04<1:23:20,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 112/2388 [04:06<1:23:17,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 113/2388 [04:09<1:23:15,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 114/2388 [04:11<1:23:12,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 115/2388 [04:13<1:23:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 116/2388 [04:15<1:23:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 117/2388 [04:17<1:23:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 118/2388 [04:20<1:23:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 119/2388 [04:22<1:23:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2388 [04:24<1:23:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.3386, 'learning_rate': 0.0001899497487437186, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2388 [04:24<1:23:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 121/2388 [04:26<1:22:59,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 122/2388 [04:28<1:22:56,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 123/2388 [04:31<1:22:53,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 124/2388 [04:33<1:22:51,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 125/2388 [04:35<1:22:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 126/2388 [04:37<1:22:47,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 127/2388 [04:39<1:22:44,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 128/2388 [04:42<1:22:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 129/2388 [04:44<1:22:39,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2388 [04:46<1:22:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2853, 'learning_rate': 0.00018911222780569514, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2388 [04:46<1:22:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 131/2388 [04:48<1:22:35,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 132/2388 [04:50<1:22:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 133/2388 [04:53<1:22:31,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 134/2388 [04:55<1:22:31,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 135/2388 [04:57<1:22:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 136/2388 [04:59<1:22:26,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 137/2388 [05:01<1:22:23,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 138/2388 [05:03<1:22:21,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 139/2388 [05:06<1:22:18,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2388 [05:08<1:22:16,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2568, 'learning_rate': 0.0001882747068676717, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2388 [05:08<1:22:16,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 141/2388 [05:10<1:22:14,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 142/2388 [05:12<1:22:12,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 143/2388 [05:14<1:22:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 144/2388 [05:17<1:22:06,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 145/2388 [05:19<1:22:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 146/2388 [05:21<1:22:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 147/2388 [05:23<1:22:00,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 148/2388 [05:25<1:21:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 149/2388 [05:28<1:21:56,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2388 [05:30<1:21:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2178, 'learning_rate': 0.00018743718592964824, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2388 [05:30<1:21:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 151/2388 [05:32<1:21:53,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 152/2388 [05:34<1:21:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 153/2388 [05:36<1:21:47,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 154/2388 [05:39<1:21:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 155/2388 [05:41<1:21:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 156/2388 [05:43<1:21:40,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 157/2388 [05:45<1:21:37,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 158/2388 [05:47<1:21:35,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 159/2388 [05:50<1:21:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2388 [05:52<1:21:32,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.1973, 'learning_rate': 0.0001865996649916248, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2388 [05:52<1:21:32,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 161/2388 [05:54<1:21:31,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 162/2388 [05:56<1:21:28,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 163/2388 [05:58<1:21:25,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 164/2388 [06:01<1:21:23,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 165/2388 [06:03<1:21:21,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 166/2388 [06:05<1:21:18,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 167/2388 [06:07<1:21:15,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 168/2388 [06:09<1:21:13,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 169/2388 [06:12<1:21:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2388 [06:14<1:21:10,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.285, 'learning_rate': 0.00018576214405360134, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2388 [06:14<1:21:10,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 171/2388 [06:16<1:21:08,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 172/2388 [06:18<1:21:06,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 173/2388 [06:20<1:21:04,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 174/2388 [06:23<1:21:02,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 175/2388 [06:25<1:20:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 176/2388 [06:27<1:20:56,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 177/2388 [06:29<1:20:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 178/2388 [06:31<1:20:51,  2.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 179/2388 [06:34<1:20:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2388 [06:36<1:20:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2457, 'learning_rate': 0.0001849246231155779, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2388 [06:36<1:20:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 181/2388 [06:38<1:20:47,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 182/2388 [06:40<1:20:44,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 183/2388 [06:42<1:20:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 184/2388 [06:44<1:20:40,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 185/2388 [06:47<1:20:38,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 186/2388 [06:49<1:20:35,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 187/2388 [06:51<1:20:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 188/2388 [06:53<1:20:31,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 189/2388 [06:55<1:20:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2388 [06:58<1:20:28,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2775, 'learning_rate': 0.00018408710217755444, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2388 [06:58<1:20:28,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 191/2388 [07:00<1:20:26,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 192/2388 [07:02<1:20:23,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 193/2388 [07:04<1:20:20,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 194/2388 [07:06<1:20:18,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 195/2388 [07:09<1:20:15,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 196/2388 [07:11<1:20:13,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 197/2388 [07:13<1:20:10,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 198/2388 [07:15<1:20:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 199/2388 [07:17<1:20:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2388 [07:20<1:20:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2982, 'learning_rate': 0.000183249581239531, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2388 [07:20<1:20:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 201/2388 [07:22<1:20:02,  2.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 202/2388 [07:24<1:20:00,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 203/2388 [07:26<1:19:57,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 204/2388 [07:28<1:19:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 205/2388 [07:31<1:19:52,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 206/2388 [07:33<1:19:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 207/2388 [07:35<1:19:47,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 208/2388 [07:37<1:19:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 209/2388 [07:39<1:19:43,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 210/2388 [07:42<1:19:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2523, 'learning_rate': 0.00018241206030150754, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m9%|▉         | 210/2388 [07:42<1:19:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 211/2388 [07:44<1:19:40,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 212/2388 [07:46<1:19:38,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 213/2388 [07:48<1:19:35,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 214/2388 [07:50<1:19:33,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 215/2388 [07:53<1:19:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 216/2388 [07:55<1:19:29,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 217/2388 [07:57<1:19:27,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 218/2388 [07:59<1:19:25,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 219/2388 [08:01<1:19:22,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 220/2388 [08:04<1:19:20,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2799, 'learning_rate': 0.0001815745393634841, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m9%|▉         | 220/2388 [08:04<1:19:20,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 221/2388 [08:06<1:19:18,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 222/2388 [08:08<1:19:16,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 223/2388 [08:10<1:19:14,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 224/2388 [08:12<1:19:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 225/2388 [08:15<1:19:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 226/2388 [08:17<1:19:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 227/2388 [08:19<1:19:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 228/2388 [08:21<1:19:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 229/2388 [08:23<1:19:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 230/2388 [08:26<1:18:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2341, 'learning_rate': 0.00018073701842546063, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 230/2388 [08:26<1:18:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 231/2388 [08:28<1:18:56,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 232/2388 [08:30<1:18:54,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 233/2388 [08:32<1:18:52,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 234/2388 [08:34<1:18:50,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 235/2388 [08:36<1:18:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 236/2388 [08:39<1:18:45,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 237/2388 [08:41<1:18:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 238/2388 [08:43<1:18:40,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 239/2388 [08:45<1:18:39,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 240/2388 [08:47<1:18:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2133, 'learning_rate': 0.0001798994974874372, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m10%|█         | 240/2388 [08:47<1:18:36,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 241/2388 [08:50<1:18:34,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 242/2388 [08:52<1:18:32,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 243/2388 [08:54<1:18:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 244/2388 [08:56<1:18:28,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 245/2388 [08:58<1:18:26,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 246/2388 [09:01<1:18:24,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 247/2388 [09:03<1:18:22,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 248/2388 [09:05<1:18:19,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 249/2388 [09:07<1:18:17,  2.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 250/2388 [09:09<1:18:15,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2531, 'learning_rate': 0.00017906197654941373, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 250/2388 [09:09<1:18:15,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 251/2388 [09:12<1:18:13,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 252/2388 [09:14<1:18:11,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 253/2388 [09:16<1:18:09,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 254/2388 [09:18<1:18:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 255/2388 [09:20<1:18:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 256/2388 [09:23<1:18:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 257/2388 [09:25<1:18:00,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 258/2388 [09:27<1:17:58,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 259/2388 [09:29<1:17:55,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 260/2388 [09:31<1:17:53,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2298, 'learning_rate': 0.0001782244556113903, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m11%|█         | 260/2388 [09:31<1:17:53,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 261/2388 [09:34<1:17:51,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 262/2388 [09:36<1:17:48,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 263/2388 [09:38<1:17:46,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 264/2388 [09:40<1:17:44,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 265/2388 [09:42<1:17:42,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 266/2388 [09:45<1:17:40,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 267/2388 [09:47<1:17:37,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 268/2388 [09:49<1:17:35,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 269/2388 [09:51<1:17:32,  2.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 270/2388 [09:53<1:17:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2248, 'learning_rate': 0.00017738693467336683, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 270/2388 [09:53<1:17:30,  2.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 281/2388 [10:18<1:17:06,  2.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 282/2388 [10:20<1:17:03,  2.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 283/2388 [10:22<1:17:01,  2.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 284/2388 [10:24<1:16:59,  2.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 285/2388 [10:26<1:16:57,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 386/2388 [14:08<1:13:14,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 387/2388 [14:10<1:13:12,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 388/2388 [14:12<1:13:10,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 389/2388 [14:15<1:13:08,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 390/2388 [14:17<1:13:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 7.2745, 'learning_rate': 0.00016733668341708543, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 390/2388 [14:17<1:13:07,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 391/2388 [14:19<1:13:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 392/2388 [14:21<1:13:02,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 393/2388 [14:23<1:13:00,  2.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 394/2388 [14:26<1:12:57,  2.20s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 395/2388 [14:28<1:12:55,  2.20s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 7B, the SageMaker training job took `XX seconds`, which is about `X hours`. The ml.p4d.24xlarge instance we used costs `$X per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
