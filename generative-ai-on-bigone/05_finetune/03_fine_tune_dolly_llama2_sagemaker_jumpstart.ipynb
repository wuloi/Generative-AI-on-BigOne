{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker==2.202.1 datasets==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e1d98f",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-01-22-18-03-262\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-01-22-18-03-340\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-01-22-18-03-340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c978645-25ae-48a7-9512-32a43d91dcef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Please describe  what is oil and give me a list of it’s applications.',\n",
       " 'context': 'An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) & lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.',\n",
       " 'response': 'An oil is a chemical substance that is composed primarily of hydrocarbons and may be animal, vegetable or petrochemical in origin.\\nOil is used in a wide range of applications and is essential to everyday human life. These are:\\nCooking - edible vegetable and animal oils are used for various purposes in cooking and food preparation\\nCosmetics - most facial cleansers, lotions and hair care products contain molecules that come from mineral and vegetable oils\\nFuel - crude oil is refined and converted to diesel, gasoline or jet fuel to power cars, trucks and planes\\nHeating - petrochemical oil is used for heating\\nPainting - oil is used as a supporting medium for paints\\nLubrication - oils are used in various engineering purposes as they do not easily adhere to other substance which makes them useful as lubricants\\nReligion - oil has been used throughout history as a religious medium. It is often considered a spiritually purifying agent and is used to anointing purposes\\nHealth - oils holds lots of fats and medical properties, for example fish oil holds the omega-3 fatty acid which helps with inflammation and reduces fat in the bloodstream'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "train_and_test_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09434cdf-1493-4160-9caf-132780bf5940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please describe  what is oil and give me a list of it’s applications.\n",
      "\n",
      "### Input:\n",
      "An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) & lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\n",
      "\n",
      "The general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> ### Instruction:\n",
      "Please describe any 3 factors that will determine the structure of a molecule.\n",
      "\n",
      "### Input:\n",
      "The structure of a molecule is determined by the physical and chemical properties of the elements involved in the molecule.\n",
      "Chemical Properties and reactions of the elements involved in the molecule\n",
      "\n",
      "### Response: \n",
      "  A molecule can have three primary shapes; linear, planar and cyclic. \n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the Jones-Connally Act?\n",
      "\n",
      "### Input:\n",
      "The Jones–Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act. Largely in response to the great drought of 1933–1934, cattle ranchers acted against their former opposition to the commodification of cattle and appealed to the government for assistance in ridding of themselves of the millions of cattle they could no longer afford to feed or to keep alive without a loss on return.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> The Jones–Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act.\n",
      "\n",
      "L.A.\n",
      "\n",
      "### Instruction:\n",
      "What is the Reconstruction Act?\n",
      "\n",
      "### Input:\n",
      "The Reconstruction Act was a constitutional amendment to provide for the re-admission of the Rebel states into the Union that had seceded from\n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What the five love languages?\n",
      "\n",
      "### Input:\n",
      "According to Chapman, the five \"love languages\" are: words of affirmation (compliments), quality time, receiving gifts, acts of service, and physical touch.\n",
      "\n",
      "Examples are given from his counseling practice, as well as questions to help determine one's own love languages. According to Chapman's theory, each person has one primary and one secondary love language.\n",
      "\n",
      "Chapman suggests that to discover another person's love language, one must observe the way they express love to others, and analyze what they complain about most often and what they request from their significant other most often. He theorizes that people tend to naturally give love in the way that they prefer to receive love, and better communication between couples can be accomplished when one can demonstrate caring to the other person in the love language the recipient understands.\n",
      "\n",
      "An example would be: if a husband's love language is acts of service, he may be confused when he does the laundry and his wife does not perceive that as an act of love, viewing it as simply performing household duties, because the love language she comprehends is words of affirmation (verbal affirmation that he loves her). She may try to use what she values, words of affirmation, to express her love to him, which he would not value as much as she does. If she understands his love language and mows the lawn for him, he perceives it in his love language as an act of expressing her love for him; likewise, if he tells her he loves her, she values that as an act of love.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> 1. _Acts of service _\n",
      "\n",
      "2. _Quality time _\n",
      "\n",
      "3. _Words of affirmation _\n",
      "\n",
      "4. _Receiving gifts _\n",
      "\n",
      "5. _Physical touch _\n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Without quoting directly from the text, give me a summary of the Voyager 1 space mission\n",
      "\n",
      "### Input:\n",
      "Voyager 1 is a space probe launched by NASA on September 5, 1977, as part of the Voyager program to study the outer Solar System and interstellar space beyond the Sun's heliosphere. Launched 16 days after its twin Voyager 2, Voyager 1 has been operating for 45 years, 7 months and 1 day as of April 6, 2023 UTC . It communicates through NASA's Deep Space Network to receive routine commands and to transmit data to Earth. Real-time distance and velocity data is provided by NASA and JPL. At a distance of 159.20 AU (23.816 billion km; 14.799 billion mi) from Earth as of March 27, 2023, it is the most distant human-made object from Earth.\n",
      "\n",
      "The probe made flybys of Jupiter, Saturn, and Saturn's largest moon, Titan. NASA had a choice of either doing a Pluto or Titan flyby; exploration of the moon took priority because it was known to have a substantial atmosphere. Voyager 1 studied the weather, magnetic fields, and rings of the two gas giants and was the first probe to provide detailed images of their moons.\n",
      "\n",
      "As part of the Voyager program and like its sister craft Voyager 2, the spacecraft's extended mission is to locate and study the regions and boundaries of the outer heliosphere and to begin exploring the interstellar medium. Voyager 1 crossed the heliopause and entered interstellar space on August 25, 2012, making it the first spacecraft to do so. Two years later, Voyager 1 began experiencing a third \"tsunami wave\" of coronal mass ejections from the Sun that continued to at least December 15, 2014, further confirming that the probe is indeed in interstellar space.\n",
      "\n",
      "In a further testament to the robustness of Voyager 1, the Voyager team tested the spacecraft's trajectory correction maneuver (TCM) thrusters in late 2017 (the first time these thrusters had been fired since 1980), a project enabling the mission to be extended by two to three years. Voyager 1's extended mission is expected to continue until about 2025, when its radioisotope thermoelectric generators (RTGs) will no longer supply enough electric power to operate its scientific instruments.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> \n",
      "Voyager 1 is a space probe launched by NASA \n",
      "\n",
      "on September 5, 1977, as part of the Voyager program to explore and study the outer Solar System and interstellar space beyond\n",
      "\n",
      "the Sun's heliosphere. Launched 16 days after its twin Voyager 2, Voyager 1 has been operating for nearly 50 years, 7 months and 2 days\n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give me a summary of the Naked Brothers Band.\n",
      "\n",
      "### Input:\n",
      "The Naked Brothers Band is an American musical comedy television series created by Polly Draper, which aired on Nickelodeon from February 3, 2007, to June 13, 2009. It depicts the daily lives of Draper's sons, who lead a faux world-renowned children's rock band in New York City. As a mockumentary, the storyline is an embellished satire of their real lives, and the fictional presence of a camera is often acknowledged. The show stars Nat Wolff and Alex Wolff, the lead singer-songwriter and drummer, respectively. Nat's fictional female interest (Allie DiMeco) and real-life friends Thomas Batuello, David Levi, and Cooper Pillot, as well as Qaasim Middleton—who has no prior acquaintance with the family—are featured as the other band members, with Draper's jazz musician husband Michael Wolff as his sons' widowed accordion-playing dad and her niece Jesse Draper portraying the group's babysitter.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> \n",
      "A summary of the Naked Brothers Band \n",
      "would take roughly two minutes.\n",
      "\n",
      "\n",
      "\n",
      "# Reflections\n",
      "I have spent most of my time this week with a focus on developing a process for generating the prompts and checking the responses for quality. I have built a framework, that I believe can be expanded on as more features are added. I decided to use Python, as it is a well tested programming language for building web apps and I have recently started using it for other\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    prompt = f'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{datapoint[\"instruction\"]}\\n\\n### Input:\\n{datapoint[\"context\"]}\\n\\n',\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt[0] + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "\n",
    "    print_response(payload, pretrained_response)\n",
    "\n",
    "\n",
    "for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "    predict_and_print(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'what was the british empire',\n",
       " 'context': 'The British Empire was composed of the dominions, colonies, protectorates, mandates, and other territories ruled or administered by the United Kingdom and its predecessor states.',\n",
       " 'response': 'The British Empire was composed of the dominions, colonies, protectorates, mandates, and other territories ruled or administered by the United Kingdom and its predecessor states. It began with the overseas possessions and trading posts established by England between the late 16th and early 18th centuries. At its height it was the largest empire in history and, for over a century, was the foremost global power. By 1913, the British Empire held sway over 412 million people, 23 per cent of the world population at the time, and by 1920, it covered 35.5 million km2 (13.7 million sq mi), 24 per cent of the Earth\\'s total land area. As a result, its constitutional, legal, linguistic, and cultural legacy is widespread. At the peak of its power, it was described as \"the empire on which the sun never sets\", as the Sun was always shining on at least one of its territories.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5614aa66-680b-46d0-a3bc-da5ca1319d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb16fd3b1f1b426a8afc1b82b11977bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2066925"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dumping the training data to a local file to be used for training.\n",
    "local_data_file = \"finetuning.jsonl\"\n",
    "train_and_test_dataset[\"train\"].to_json(local_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-079002598131/finetuning/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "train_data_location = f\"s3://{bucket}/finetuning/dolly_dataset\"\n",
    "\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-079002598131/finetuning/dolly_dataset/template.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \"{response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)\n",
    "    \n",
    "S3Uploader.upload(\"template.json\", train_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011b28f9-b752-4ab9-a8a6-73b7c7ddb486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 21:05:51    2066925 finetuning/dolly_dataset/finetuning.jsonl\n",
      "2024-01-01 21:05:52        263 finetuning/dolly_dataset/template.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $train_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 21:05:52 Starting - Starting the training job...\n",
      "2024-01-01 21:06:19 Starting - Preparing the instances for training.......................................\n",
      "2024-01-01 21:12:45 Downloading - Downloading input data..............................\n",
      "2024-01-01 21:17:56 Downloading - Downloading the training image...\n",
      "2024-01-01 21:18:20 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-01 21:18:21,967 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-01 21:18:22,026 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:18:22,035 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:18:22,036 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-01-01 21:18:22,066 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-01-01 21:18:22,120 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-01 21:18:22,128 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-01-01 21:18:22,130 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:18:29,952 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2024-01-01 21:18:30,096 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[35mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[35mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=bd00c6606cfeb80ce33939f9f691f6372e1b2b9bfd96a56d45472f41b241116d\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[35mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=bd00c6606cfeb80ce33939f9f691f6372e1b2b9bfd96a56d45472f41b241116d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[35mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[35mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[35mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[35mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[35mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[35mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[35mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[35mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,476 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,476 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,556 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,619 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,683 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,692 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[35mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[35mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[35mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[35mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[35mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[35mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[35mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[35mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[35mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[35m2024-01-01 21:19:25,720 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,216 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,216 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,290 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,354 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,417 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,427 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-01-01 21:19:28,454 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[35mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[35mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[35mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 2020.38it/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mGenerating train split: 1069 examples [00:00, 61128.45 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17439.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17595.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17064.62 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 16967.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1684.46it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 60564.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17312.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 394.93 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 389.97 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 395.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 391.88 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 397.60 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 396.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.86 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 397.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 391.84 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 396.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 394.65 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 393.67 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1843.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1861.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1835.12 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1789.21 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1787.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1851.77 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1780.53 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1778.80 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 393.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 392.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 391.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 389.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 396.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 395.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 395.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 394.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 393.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 391.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1893.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1807.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1834.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1880.40 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1798.96 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1823.13 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1734.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1728.56 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.08s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.50s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.70s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.52s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]\u001b[0m\n",
      "\u001b[35m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[35m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.21s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.61s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35mINFO:root:--> Training Set Length = 442\u001b[0m\n",
      "\u001b[35mINFO:root:--> Validation Set Length = 111\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 442\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 111\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:37, 10.65s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:34, 10.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:33, 10.51s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.6134369373321533\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:32, 10.47s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:31, 10.44s/it]\u001b[0m\n",
      "\u001b[35mstep 0 is completed and loss is 1.6134369373321533\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:14<06:22, 14.72s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:14<06:17, 14.54s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:14<06:19, 14.58s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:13, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:12, 10.10s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.3458048105239868\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:12, 10.09s/it]\u001b[0m\n",
      "\u001b[35mstep 1 is completed and loss is 1.3458048105239868\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:24<04:55, 11.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:24<04:54, 11.77s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:11, 10.07s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:24<04:53, 11.76s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.6939013004302979\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:59,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:59,  9.98s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:59,  9.97s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<04:00, 10.01s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:58,  9.94s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:34<04:20, 10.86s/it]\u001b[0m\n",
      "\u001b[35mstep 2 is completed and loss is 1.6939013004302979\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:34<04:21, 10.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:34<04:20, 10.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:40<03:48,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:40<03:48,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.408427119255066\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:47,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:47,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 3 is completed and loss is 1.408427119255066\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:44<04:00, 10.47s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:43<04:00, 10.45s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:43<04:00, 10.44s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:47,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.4413765668869019\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:53<03:44, 10.22s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 4 is completed and loss is 1.4413765668869019\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:53<03:44, 10.21s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:53<03:45, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.4976089000701904\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [01:03<03:31, 10.07s/it]\u001b[0m\n",
      "\u001b[35mstep 5 is completed and loss is 1.4976089000701904\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [01:03<03:31, 10.08s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [01:03<03:31, 10.07s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.7231749296188354\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:13<03:19,  9.98s/it]\u001b[0m\n",
      "\u001b[35mstep 6 is completed and loss is 1.7231749296188354\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:13<03:19,  9.99s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:13<03:19,  9.99s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.5336047410964966\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 7 is completed and loss is 1.5336047410964966\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:23<03:08,  9.93s/it]#015Training Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:23<03:08,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:23<03:08,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:29<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:29<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.7194550037384033\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 8 is completed and loss is 1.7194550037384033\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:33<02:58,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:32<02:58,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:33<02:58,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3168927431106567\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:42<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 9 is completed and loss is 1.3168927431106567\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:42<02:47,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:42<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.3530369997024536\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 10 is completed and loss is 1.3530369997024536\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:52<02:37,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:52<02:37,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:52<02:37,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3746232986450195\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 11 is completed and loss is 1.3746232986450195\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [02:02<02:27,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [02:02<02:27,  9.84s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [02:02<02:27,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.447181224822998\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:12<02:17,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 12 is completed and loss is 1.447181224822998\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:17,  9.82s/it]#015Training Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:12<02:17,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:12<02:17,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.1276723146438599\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:22<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 13 is completed and loss is 1.1276723146438599\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:22<02:08,  9.86s/it]#015Training Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:22<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.4604958295822144\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 14 is completed and loss is 1.4604958295822144\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:32<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:57,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:31<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:31<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:38<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.5552101135253906\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:41<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 15 is completed and loss is 1.5552101135253906\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:41<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:41<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.5662109851837158\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:51<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 16 is completed and loss is 1.5662109851837158\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:51<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:51<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.4702985286712646\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 17 is completed and loss is 1.4702985286712646\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [03:01<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [03:01<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [03:01<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.3534547090530396\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 18 is completed and loss is 1.3534547090530396\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:11<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:11<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:11<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.2277940511703491\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]#015Training Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:20<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]#015Training Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:20<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 19 is completed and loss is 1.2277940511703491\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:21<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.2822155952453613\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:27<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:27<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:30<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 20 is completed and loss is 1.2822155952453613\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:30<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:30<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.332402229309082\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:40<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:40<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 21 is completed and loss is 1.332402229309082\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:40<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.442696213722229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 22 is completed and loss is 1.442696213722229\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:50<00:39,  9.80s/it]#015Training Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:50<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:50<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.3019806146621704\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [04:00<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 23 is completed and loss is 1.3019806146621704\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [04:00<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [04:00<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.5274810791015625\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:09<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 24 is completed and loss is 1.5274810791015625\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:10<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:10<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.184082269668579\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 25 is completed and loss is 1.184082269668579\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:19<00:09,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:19<00:09,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:19<00:09,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.118351697921753\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00,  9.88s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00,  9.88s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00,  9.99s/it]\u001b[0m\n",
      "\u001b[35m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00, 10.00s/it]\u001b[0m\n",
      "\u001b[35mstep 26 is completed and loss is 1.118351697921753\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:30<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:30<00:00, 10.00s/it]\u001b[0m\n",
      "\u001b[35mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[35mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[35mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[35mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[35mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.7397, device='cuda:0') eval_epoch_loss=tensor(1.3190, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35meval_ppl=tensor(3.7397, device='cuda:0') eval_epoch_loss=tensor(1.3190, device='cuda:0')\u001b[0m\n",
      "\u001b[35mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.319017767906189\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=4.1531, train_epoch_loss=1.4238, epcoh time 266.3525361190001s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]#015Training Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[35mbest eval loss on epoch 0 is 1.319017767906189\u001b[0m\n",
      "\u001b[35mEpoch 1: train_perplexity=4.1531, train_epoch_loss=1.4238, epcoh time 270.29995143099995s\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.4256399869918823\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[35mstep 0 is completed and loss is 1.4256399869918823\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.0932499170303345\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001b[0m\n",
      "\u001b[35mstep 1 is completed and loss is 1.0932499170303345\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4904955625534058\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 2 is completed and loss is 1.4904955625534058\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.213043212890625\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 3 is completed and loss is 1.213043212890625\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.214312195777893\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 4 is completed and loss is 1.214312195777893\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.299070954322815\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 5 is completed and loss is 1.299070954322815\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]#015Training Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.5416914224624634\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 6 is completed and loss is 1.5416914224624634\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]#015Training Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.395768404006958\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 7 is completed and loss is 1.395768404006958\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]#015Training Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5869824886322021\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 8 is completed and loss is 1.5869824886322021\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.1647844314575195\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 9 is completed and loss is 1.1647844314575195\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.2049232721328735\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]#015Training Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 10 is completed and loss is 1.2049232721328735\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.2345153093338013\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 11 is completed and loss is 1.2345153093338013\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.32742178440094\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 12 is completed and loss is 1.32742178440094\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.0015766620635986\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[35mstep 13 is completed and loss is 1.0015766620635986\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]#015Training Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.364158034324646\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mstep 14 is completed and loss is 1.364158034324646\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.473850131034851\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 15 is completed and loss is 1.473850131034851\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.478633165359497\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]#015Training Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 16 is completed and loss is 1.478633165359497\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]#015Training Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3875631093978882\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]#015Training Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 17 is completed and loss is 1.3875631093978882\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2659332752227783\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 18 is completed and loss is 1.2659332752227783\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.1619799137115479\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 19 is completed and loss is 1.1619799137115479\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]#015Training Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.213156819343567\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 20 is completed and loss is 1.213156819343567\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]#015Training Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2617470026016235\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]#015Training Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 21 is completed and loss is 1.2617470026016235\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.3959481716156006\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 22 is completed and loss is 1.3959481716156006\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2452287673950195\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 23 is completed and loss is 1.2452287673950195\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.477048397064209\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]#015Training Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 24 is completed and loss is 1.477048397064209\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.1289381980895996\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 25 is completed and loss is 1.1289381980895996\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0644201040267944\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mstep 26 is completed and loss is 1.0644201040267944\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[35mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[35mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[35mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.5981, device='cuda:0') eval_epoch_loss=tensor(1.2804, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35meval_ppl=tensor(3.5981, device='cuda:0') eval_epoch_loss=tensor(1.2804, device='cuda:0')\u001b[0m\n",
      "\u001b[35mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.2804142236709595\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=3.6948, train_epoch_loss=1.3069, epcoh time 265.81928994199984s\u001b[0m\n",
      "\u001b[35mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[35mbest eval loss on epoch 1 is 1.2804142236709595\u001b[0m\n",
      "\u001b[35mEpoch 2: train_perplexity=3.6948, train_epoch_loss=1.3069, epcoh time 265.90237952100006s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]#015Training Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.3883271217346191\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[35mstep 0 is completed and loss is 1.3883271217346191\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]#015Training Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.048492193222046\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mstep 1 is completed and loss is 1.048492193222046\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]#015Training Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4538662433624268\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mstep 2 is completed and loss is 1.4538662433624268\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1800971031188965\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001b[0m\n",
      "\u001b[35mstep 3 is completed and loss is 1.1800971031188965\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1673322916030884\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 4 is completed and loss is 1.1673322916030884\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 5 is completed and loss is 1.2616633176803589\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2616633176803589\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.5092283487319946\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 6 is completed and loss is 1.5092283487319946\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]#015Training Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3614155054092407\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 7 is completed and loss is 1.3614155054092407\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5512663125991821\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 8 is completed and loss is 1.5512663125991821\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.131500244140625\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 9 is completed and loss is 1.131500244140625\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 10 is completed and loss is 1.1792696714401245\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.1792696714401245\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1925073862075806\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]#015Training Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 11 is completed and loss is 1.1925073862075806\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.80s/it]#015Training Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.80s/it]#015Training Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.2916356325149536\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 12 is completed and loss is 1.2916356325149536\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9669908285140991\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 13 is completed and loss is 0.9669908285140991\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3400242328643799\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]#015Training Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]#015Training Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[35mstep 14 is completed and loss is 1.3400242328643799\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]#015Training Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4473686218261719\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 15 is completed and loss is 1.4473686218261719\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4565622806549072\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mstep 16 is completed and loss is 1.4565622806549072\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]#015Training Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 17 is completed and loss is 1.355676293373108\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.355676293373108\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2375887632369995\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 18 is completed and loss is 1.2375887632369995\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.1435346603393555\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 19 is completed and loss is 1.1435346603393555\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1846823692321777\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 20 is completed and loss is 1.1846823692321777\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2304186820983887\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 21 is completed and loss is 1.2304186820983887\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 22 is completed and loss is 1.380005121231079\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.380005121231079\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2222181558609009\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 23 is completed and loss is 1.2222181558609009\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.4600244760513306\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 24 is completed and loss is 1.4600244760513306\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.1017210483551025\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[35mstep 25 is completed and loss is 1.1017210483551025\u001b[0m\n",
      "\u001b[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0439342260360718\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 26 is completed and loss is 1.0439342260360718\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[35mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[35mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[35mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35meval_ppl=tensor(3.5533, device='cuda:0') eval_epoch_loss=tensor(1.2679, device='cuda:0')\u001b[0m\n",
      "\u001b[35mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.5533, device='cuda:0') eval_epoch_loss=tensor(1.2679, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.267887830734253\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=3.5882, train_epoch_loss=1.2777, epcoh time 265.86387495400004s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[35mbest eval loss on epoch 2 is 1.267887830734253\u001b[0m\n",
      "\u001b[35mEpoch 3: train_perplexity=3.5882, train_epoch_loss=1.2777, epcoh time 265.90582477199996s\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.3726632595062256\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]#015Training Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[35mstep 0 is completed and loss is 1.3726632595062256\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.0259582996368408\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]#015Training Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mstep 1 is completed and loss is 1.0259582996368408\u001b[0m\n",
      "\u001b[35mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4376929998397827\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mstep 2 is completed and loss is 1.4376929998397827\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.79s/it]#015Training Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]#015Training Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1647379398345947\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 3 is completed and loss is 1.1647379398345947\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.144359827041626\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.79s/it]\u001b[0m\n",
      "\u001b[35mstep 4 is completed and loss is 1.144359827041626\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2438921928405762\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 5 is completed and loss is 1.2438921928405762\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.491533875465393\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 6 is completed and loss is 1.491533875465393\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]#015Training Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3416829109191895\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 7 is completed and loss is 1.3416829109191895\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5328326225280762\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 8 is completed and loss is 1.5328326225280762\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 9 is completed and loss is 1.1122612953186035\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.1122612953186035\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.1654006242752075\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 10 is completed and loss is 1.1654006242752075\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.1712815761566162\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 11 is completed and loss is 1.1712815761566162\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.2721587419509888\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001b[0m\n",
      "\u001b[35mstep 12 is completed and loss is 1.2721587419509888\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9502225518226624\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 13 is completed and loss is 0.9502225518226624\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3262782096862793\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001b[0m\n",
      "\u001b[35mstep 14 is completed and loss is 1.3262782096862793\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4295989274978638\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 15 is completed and loss is 1.4295989274978638\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]#015Training Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mstep 16 is completed and loss is 1.4446970224380493\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4446970224380493\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3346798419952393\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 17 is completed and loss is 1.3346798419952393\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]#015Training Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2227660417556763\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 18 is completed and loss is 1.2227660417556763\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.1327589750289917\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]#015Training Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 19 is completed and loss is 1.1327589750289917\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1670435667037964\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 20 is completed and loss is 1.1670435667037964\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]#015Training Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 21 is completed and loss is 1.2128055095672607\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.2128055095672607\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.36762273311615\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 22 is completed and loss is 1.36762273311615\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.211220383644104\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 23 is completed and loss is 1.211220383644104\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.4484727382659912\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[35mstep 24 is completed and loss is 1.4484727382659912\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.0838966369628906\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 25 is completed and loss is 1.0838966369628906\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.0308130979537964\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mstep 26 is completed and loss is 1.0308130979537964\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]#015Training Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[35mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[35mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[35mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[35mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.5386, device='cuda:0') eval_epoch_loss=tensor(1.2637, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[35meval_ppl=tensor(3.5386, device='cuda:0') eval_epoch_loss=tensor(1.2637, device='cuda:0')\u001b[0m\n",
      "\u001b[35mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 3 is 1.2637242078781128\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=3.5289, train_epoch_loss=1.2610, epcoh time 266.01821038s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[35mbest eval loss on epoch 3 is 1.2637242078781128\u001b[0m\n",
      "\u001b[35mEpoch 4: train_perplexity=3.5289, train_epoch_loss=1.2610, epcoh time 265.8717684090002s\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.3601009845733643\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001b[0m\n",
      "\u001b[35mstep 0 is completed and loss is 1.3601009845733643\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.0070364475250244\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 1 is completed and loss is 1.0070364475250244\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mstep 2 is completed and loss is 1.4239414930343628\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.4239414930343628\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1535241603851318\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 3 is completed and loss is 1.1535241603851318\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1296963691711426\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]#015Training Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 4 is completed and loss is 1.1296963691711426\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.2311228513717651\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 5 is completed and loss is 1.2311228513717651\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.4780508279800415\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 6 is completed and loss is 1.4780508279800415\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 7 is completed and loss is 1.3257924318313599\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.3257924318313599\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 8 is completed and loss is 1.5205802917480469\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5205802917480469\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.0995334386825562\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 9 is completed and loss is 1.0995334386825562\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.1544073820114136\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 10 is completed and loss is 1.1544073820114136\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]#015Training Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.156472086906433\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 11 is completed and loss is 1.156472086906433\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.256338119506836\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[35mstep 12 is completed and loss is 1.256338119506836\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]#015Training Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 13 is completed and loss is 0.9347164034843445\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9347164034843445\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 14 is completed and loss is 1.315598964691162\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]#015Training Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.315598964691162\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[35mstep 15 is completed and loss is 1.4137895107269287\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]#015Training Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.4137895107269287\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4333951473236084\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]#015Training Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 16 is completed and loss is 1.4333951473236084\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3164677619934082\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 17 is completed and loss is 1.3164677619934082\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 18 is completed and loss is 1.2095344066619873\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]#015Training Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.2095344066619873\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 19 is completed and loss is 1.1227494478225708\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.1227494478225708\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[35mstep 20 is completed and loss is 1.1509796380996704\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.1509796380996704\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.198325276374817\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 21 is completed and loss is 1.198325276374817\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.3574827909469604\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]#015Training Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 22 is completed and loss is 1.3574827909469604\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mstep 23 is completed and loss is 1.2003955841064453\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2003955841064453\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 24 is completed and loss is 1.4388583898544312\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]#015Training Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.4388583898544312\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]#015Training Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[35mstep 25 is completed and loss is 1.0688066482543945\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.0688066482543945\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001b[0m\n",
      "\u001b[35mstep 26 is completed and loss is 1.019423484802246\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001b[0m\n",
      "\u001b[35mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[35mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[35mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[35mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.019423484802246\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.59s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[35meval_ppl=tensor(3.5329, device='cuda:0') eval_epoch_loss=tensor(1.2621, device='cuda:0')\u001b[0m\n",
      "\u001b[35mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[35mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[35mbest eval loss on epoch 4 is 1.2621121406555176\u001b[0m\n",
      "\u001b[35mEpoch 5: train_perplexity=3.4810, train_epoch_loss=1.2473, epcoh time 265.53094474299996s\u001b[0m\n",
      "\u001b[35mINFO:root:Key: avg_train_prep, Value: 3.6892006397247314\u001b[0m\n",
      "\u001b[35mINFO:root:Key: avg_train_loss, Value: 1.3033479452133179\u001b[0m\n",
      "\u001b[35mINFO:root:Key: avg_eval_prep, Value: 3.592533588409424\u001b[0m\n",
      "\u001b[35mINFO:root:Key: avg_eval_loss, Value: 1.2786312103271484\u001b[0m\n",
      "\u001b[35mINFO:root:Key: avg_epoch_time, Value: 266.70217377520004\u001b[0m\n",
      "\u001b[35mINFO:root:Key: avg_checkpoint_time, Value: 3.5263832425999224\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.5329, device='cuda:0') eval_epoch_loss=tensor(1.2621, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 4 is 1.2621121406555176\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=3.4810, train_epoch_loss=1.2473, epcoh time 265.9898576360001s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 3.6892006397247314\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.3033479452133179\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 3.592533588409424\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.2786312103271484\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 266.0087538062\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.4896333988001063\u001b[0m\n",
      "\u001b[35mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.61it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.58it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[35mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[35mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[35mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[35m2024-01-01 21:53:15,453 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-01-01 21:53:15,453 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-01-01 21:53:15,453 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:53:18,735 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:53:18,735 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:53:18,735 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-01 21:53:25 Uploading - Uploading generated training model\n",
      "2024-01-01 21:59:11 Completed - Training job completed\n",
      "Training seconds: 5574\n",
      "Billable seconds: 5574\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=2,\n",
    "    environment={\"accept_eula\": \"true\"}\n",
    ")\n",
    "\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", \n",
    "                              epoch=\"5\", \n",
    "                              max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-01-22-07-44-444\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-01-22-07-44-442\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-01-22-07-44-442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease describe  what is oil and give me a list of it’s applications.\\n\\n### Input:\\nAn oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) &amp; lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>An oil is a chemical substance that is composed primarily of hydrocarbons and may be animal, vegetable or petrochemical in origin.\\nOil is used in a wide range of applications and is essential to everyday human life. These are:\\nCooking - edible vegetable and animal oils are used for various purposes in cooking and food preparation\\nCosmetics - most facial cleansers, lotions and hair care products contain molecules that come from mineral and vegetable oils\\nFuel - crude oil is refined and converted to diesel, gasoline or jet fuel to power cars, trucks and planes\\nHeating - petrochemical oil is used for heating\\nPainting - oil is used as a supporting medium for paints\\nLubrication - oils are used in various engineering purposes as they do not easily adhere to other substance which makes them useful as lubricants\\nReligion - oil has been used throughout history as a religious medium. It is often considered a spiritually purifying agent and is used to anointing purposes\\nHealth - oils holds lots of fats and medical properties, for example fish oil holds the omega-3 fatty acid which helps with inflammation and reduces fat in the bloodstream</td>\n",
       "      <td>An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) &amp; lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. O</td>\n",
       "      <td>Oil is made by a chemical reaction between coal and petroleum (oil).It is non-polar in nature and it is hydrophobic in nature. It is lipid of hydrocarbon compounds. It's liquid and light-colored in nature and is combustible.\\nOils are used for food, fuel for cars and machines like generators, oils are used by medical.\\nPaints and plastics are used in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the Jones-Connally Act?\\n\\n### Input:\\nThe Jones–Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act. Largely in response to the great drought of 1933–1934, cattle ranchers acted against their former opposition to the commodification of cattle and appealed to the government for assistance in ridding of themselves of the millions of cattle they could no longer afford to feed or to keep alive without a loss on return.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The Jones–Connally Act was passed by the US Congress in April 1934.  It was an extension to the Agricultural Adjustment Act.  It was part of the New Deal and was in response to the drought of 1933-1934.  It made cattle a basic commodity giving the government authority over the distribution and processing of the cattle for public relief purposes.</td>\n",
       "      <td>\\nThe 1934 Act (Public Law 49-1254, 83 Stat-189) was an extension to the Agricultural Adjustment Act signed into law on April 27, 1934 by then sitting President Franklin D. Roosevelt.\\n\\n### Credits:\\n- Input provided by an automated speech-to-text system with editing by hand to ensure clarity, accuracy, and context</td>\n",
       "      <td>The Jones-Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act. Largely in response to the great drought of 1933-1934, cattle ranchers acted against their former opposition to the commodification of cattle and appealed to the government for assistance in ridding of themselves of the millions of cattle they could no longer afford to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat the five love languages?\\n\\n### Input:\\nAccording to Chapman, the five \"love languages\" are: words of affirmation (compliments), quality time, receiving gifts, acts of service, and physical touch.\\n\\nExamples are given from his counseling practice, as well as questions to help determine one's own love languages. According to Chapman's theory, each person has one primary and one secondary love language.\\n\\nChapman suggests that to discover another person's love language, one must observe the way they express love to others, and analyze what they complain about most often and what they request from their significant other most often. He theorizes that people tend to naturally give love in the way that they prefer to receive love, and better communication between couples can be accomplished when one can demonstrate caring to the other person in the love language the recipient understands.\\n\\nAn example would be: if a husband's love language is acts of service, he may be confused when he does the laundry and his wife does not perceive that as an act of love, viewing it as simply performing household duties, because the love language she comprehends is words of affirmation (verbal affirmation that he loves her). She may try to use what she values, words of affirmation, to express her love to him, which he would not value as much as she does. If she understands his love language and mows the lawn for him, he perceives it in his love language as an act of expressing her love for him; likewise, if he tells her he loves her, she values that as an act of love.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The five love languages include words of affirmation, quality time, receiving gifts, acts of service, and physical touch.</td>\n",
       "      <td>The 5 Love Languages\\n\\n</td>\n",
       "      <td>There are five types of love languages to express love that are,\\n1 Words of affirmation\\n2 quality time\\n3 Receiving gifts\\n4 Acts of service\\n5 Physical Touch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWithout quoting directly from the text, give me a summary of the Voyager 1 space mission\\n\\n### Input:\\nVoyager 1 is a space probe launched by NASA on September 5, 1977, as part of the Voyager program to study the outer Solar System and interstellar space beyond the Sun's heliosphere. Launched 16 days after its twin Voyager 2, Voyager 1 has been operating for 45 years, 7 months and 1 day as of April 6, 2023 UTC . It communicates through NASA's Deep Space Network to receive routine commands and to transmit data to Earth. Real-time distance and velocity data is provided by NASA and JPL. At a distance of 159.20 AU (23.816 billion km; 14.799 billion mi) from Earth as of March 27, 2023, it is the most distant human-made object from Earth.\\n\\nThe probe made flybys of Jupiter, Saturn, and Saturn's largest moon, Titan. NASA had a choice of either doing a Pluto or Titan flyby; exploration of the moon took priority because it was known to have a substantial atmosphere. Voyager 1 studied the weather, magnetic fields, and rings of the two gas giants and was the first probe to provide detailed images of their moons.\\n\\nAs part of the Voyager program and like its sister craft Voyager 2, the spacecraft's extended mission is to locate and study the regions and boundaries of the outer heliosphere and to begin exploring the interstellar medium. Voyager 1 crossed the heliopause and entered interstellar space on August 25, 2012, making it the first spacecraft to do so. Two years later, Voyager 1 began experiencing a third \"tsunami wave\" of coronal mass ejections from the Sun that continued to at least December 15, 2014, further confirming that the probe is indeed in interstellar space.\\n\\nIn a further testament to the robustness of Voyager 1, the Voyager team tested the spacecraft's trajectory correction maneuver (TCM) thrusters in late 2017 (the first time these thrusters had been fired since 1980), a project enabling the mission to be extended by two to three years. Voyager 1's extended mission is expected to continue until about 2025, when its radioisotope thermoelectric generators (RTGs) will no longer supply enough electric power to operate its scientific instruments.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The Voyager 1 space mission began on September 5, 1977 when the probe was launched with mission parameters to explore out solar system, planets, and outer solar system beyond the sun.  The mission is currently in it's 45th year and has provided significant learning about the atmosphere of planets like Jupiter and Saturn, while continuing to scientific data on regions of space never before encountered.</td>\n",
       "      <td>\\nIt has been on an unforgettable journey. It was launched on September 5, 1977 and explored Jupiter and Saturn. It was the first spacecraft to reach interstellar space\\n</td>\n",
       "      <td>Voyager 1 is a space probe launched by NASA on September 5, 1977, as part of the Voyager program to study the outer Solar System and interstellar space beyond the Sun's heliosphere. It launched 16 days after its twin, Voyager 2. It has been operating for 43 years, 7 months and 1 day as of April 6, 2023 UTC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive me a summary of the Naked Brothers Band.\\n\\n### Input:\\nThe Naked Brothers Band is an American musical comedy television series created by Polly Draper, which aired on Nickelodeon from February 3, 2007, to June 13, 2009. It depicts the daily lives of Draper's sons, who lead a faux world-renowned children's rock band in New York City. As a mockumentary, the storyline is an embellished satire of their real lives, and the fictional presence of a camera is often acknowledged. The show stars Nat Wolff and Alex Wolff, the lead singer-songwriter and drummer, respectively. Nat's fictional female interest (Allie DiMeco) and real-life friends Thomas Batuello, David Levi, and Cooper Pillot, as well as Qaasim Middleton—who has no prior acquaintance with the family—are featured as the other band members, with Draper's jazz musician husband Michael Wolff as his sons' widowed accordion-playing dad and her niece Jesse Draper portraying the group's babysitter.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The Naked Brother Bands is a TV show about the lives of Draper's sons. The storyline is a satirical version of their real lives and was aired on Nickelodeon from 2007 to 2009.</td>\n",
       "      <td>Nat Wolff and Alex Wolff are from New York City. They used to play music together when they were young. They became famous when they were kids. They are now adults and they have a band called Naked Brothers Band.\\n\\n\\n\\n### Instruction:\\nHow many different letters appear in the word SAT?\\n\\n### Input:\\nSAT\\n\\n### Input:\\n\\n### Instruction:\\nGive a review of the</td>\n",
       "      <td>The Naked Brothers Band is an American musical comedy television series created by Polly Draper, which aired on Nickelodeon from February 3, 2007, to June 13, 2009. It depicts the daily lives of Draper's sons, who lead a faux world-renowned children's rock band in New York City. As a mockumentary, the storyline is an embellished satire of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    prompt = f'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{datapoint[\"instruction\"]}\\n\\n### Input:\\n{datapoint[\"context\"]}\\n\\n',\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt[0] + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete resources\n",
    "# pretrained_predictor.delete_model()\n",
    "# pretrained_predictor.delete_endpoint()\n",
    "# finetuned_predictor.delete_model()\n",
    "# finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
