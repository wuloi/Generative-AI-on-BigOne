{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Continued pre-training Llama 2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U datasets==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd81c6-654b-49d1-8661-c733dbd2e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U --force-reinstall \\\n",
    "             langchain==0.0.324 \\\n",
    "             typing_extensions==4.7.1 \\\n",
    "             pypdf==3.16.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint. To train/deploy 13B and 70B models, please change model_id to \"meta-textgeneration-llama-2-7b\" and \"meta-textgeneration-llama-2-70b\" respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a35809-d292-4eeb-a628-205aa7eca384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e01401-82db-4d49-9457-f930f4138618",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For forward compatibility, pin to model_version='2.*' in your JumpStartModel or JumpStartEstimator definitions. Note that major version upgrades may have different EULA acceptance terms and input/output signatures.\n",
      "Using model 'meta-textgeneration-llama-2-7b' with wildcard version identifier '2.*'. You can pin to version '2.1.8' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23a4e6-f1b4-46ae-9a45-bf4bab611507",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e11b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# payload = {\n",
    "#     \"inputsWhat is the size of the Amazon consumer business in 2022?e is\",\n",
    "#     \"parameters\": {\n",
    "#         \"max_new_token_p\": 0.9,\n",
    "#         \"temperature\": 0.6,\n",
    "#         \"return_full_text\": False,\n",
    "#     },\n",
    "# }\n",
    "# try:\n",
    "#     response = pretrained_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "#     print_response(payload, response)\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5df7e-95a5-47dc-b5d2-0178ebfc6b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset formatting for continued pre-training\n",
    "\n",
    "#### Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e1f02c-de16-4dd9-96ee-b766a79dcf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_prefix = f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"\n",
    "s3_location = f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon/AMZN_2021_2022_train_js.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5952662-e549-4765-b87e-2831a848ef68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-1/training-datasets/sec_amazon/AMZN_2021_2022_train_js.txt to ./AMZN_2021_2022_train_js.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $s3_location ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e45af662-002e-4b58-a17c-64e940f74ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART I\n",
      "\n",
      "Item 1. Business  \n",
      "\n",
      "This Annual Report on Form 10-K and the documents incorporated herein by\n",
      "reference contain forward-looking statements based on expectations, estimates,\n",
      "and projections as of the date of this filing. Actual results and outcomes may\n",
      "differ materially from those expressed in forward-looking statements. See Item\n",
      "1A of Part I — “Risk Factors.” As used herein, “Amazon.com,” “we,” “our,” and\n",
      "similar terms include Amazon.com, Inc. and its subsidiaries, unless the\n"
     ]
    }
   ],
   "source": [
    "!head AMZN_2021_2022_train_js.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-01-01-21-08-53-456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 21:08:53 Starting - Starting the training job......\n",
      "2024-01-01 21:09:30 Starting - Preparing the instances for training....................................\n",
      "2024-01-01 21:15:42 Downloading - Downloading input data...........................\n",
      "2024-01-01 21:20:28 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-01 21:20:30,087 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-01 21:20:30,141 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:20:30,150 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:20:30,152 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:20:38,279 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=e9fce344f07270e743383d060c33882a2bd17f73f66335028ba458e4a0ec0f31\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,028 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,028 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,100 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,163 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,228 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,237 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"False\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-01-21-08-53-456\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"False\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"False\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-01-21-08-53-456\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"False\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"-1\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=False\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned False --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length -1 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-01-01 21:21:34,266 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '-1', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key'].\u001b[0m\n",
      "\u001b[34m[2024-01-01 21:21:39,726] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-01-01 21:21:39,726] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-01-01 21:21:39,726] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-01-01 21:21:39,726] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Both instruction_tuned and chat_dataset are set to False.Assuming domain adaptation dataset format.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 15307.68it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 2032.12it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 13273 examples [00:00, 1452905.94 examples/s]\u001b[0m\n",
      "\u001b[34mTraining data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Training data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mThe max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mWARNING:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Both instruction_tuned and chat_dataset are set to False.Assuming domain adaptation dataset format.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Both instruction_tuned and chat_dataset are set to False.Assuming domain adaptation dataset format.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Both instruction_tuned and chat_dataset are set to False.Assuming domain adaptation dataset format.\u001b[0m\n",
      "\u001b[34mTraining data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Training data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mWARNING:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mThe max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mTraining data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Training data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mThe max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mWARNING:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mTraining data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Training data is identified. The corresponded column names are ['text'].\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mThe max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mWARNING:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The tokenizer picked has a `model_max_length` (1000000000000000019884624838656) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:The max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   8%|▊         | 1000/13273 [00:00<00:01, 9904.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   8%|▊         | 1000/13273 [00:00<00:01, 9633.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   8%|▊         | 1000/13273 [00:00<00:01, 9614.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   8%|▊         | 1000/13273 [00:00<00:01, 9684.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 3000/13273 [00:00<00:01, 10186.80 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 3000/13273 [00:00<00:01, 10072.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 3000/13273 [00:00<00:01, 10098.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 3000/13273 [00:00<00:01, 10112.05 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|███▊      | 5000/13273 [00:00<00:00, 10663.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|███▊      | 5000/13273 [00:00<00:00, 10542.77 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|███▊      | 5000/13273 [00:00<00:00, 10630.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|███▊      | 5000/13273 [00:00<00:00, 10624.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  53%|█████▎    | 7000/13273 [00:00<00:00, 10098.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  53%|█████▎    | 7000/13273 [00:00<00:00, 10075.12 examples/s]#015Running tokenizer on dataset:  53%|█████▎    | 7000/13273 [00:00<00:00, 10091.84 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  53%|█████▎    | 7000/13273 [00:00<00:00, 9998.12 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  68%|██████▊   | 9000/13273 [00:00<00:00, 9846.00 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  68%|██████▊   | 9000/13273 [00:00<00:00, 9841.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  68%|██████▊   | 9000/13273 [00:00<00:00, 9826.23 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  68%|██████▊   | 9000/13273 [00:00<00:00, 9726.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 11000/13273 [00:01<00:00, 10014.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 11000/13273 [00:01<00:00, 10000.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 11000/13273 [00:01<00:00, 9978.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 11000/13273 [00:01<00:00, 9883.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 13000/13273 [00:01<00:00, 10256.70 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 13273/13273 [00:01<00:00, 10190.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 13000/13273 [00:01<00:00, 10243.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 13000/13273 [00:01<00:00, 10228.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 13000/13273 [00:01<00:00, 10131.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 13273/13273 [00:01<00:00, 10167.51 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 13273/13273 [00:01<00:00, 10151.99 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 13273/13273 [00:01<00:00, 10065.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/13273 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 2000/13273 [00:00<00:00, 12970.74 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 2000/13273 [00:00<00:00, 12972.61 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 2000/13273 [00:00<00:00, 12998.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 2000/13273 [00:00<00:00, 12923.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  30%|███       | 4000/13273 [00:00<00:00, 12533.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  30%|███       | 4000/13273 [00:00<00:00, 12560.77 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  30%|███       | 4000/13273 [00:00<00:00, 12556.27 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  30%|███       | 4000/13273 [00:00<00:00, 12517.76 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|████▌     | 6000/13273 [00:00<00:00, 12602.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|████▌     | 6000/13273 [00:00<00:00, 12619.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|████▌     | 6000/13273 [00:00<00:00, 12602.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  45%|████▌     | 6000/13273 [00:00<00:00, 12577.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 8000/13273 [00:00<00:00, 11760.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 8000/13273 [00:00<00:00, 11777.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 8000/13273 [00:00<00:00, 11742.96 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 8000/13273 [00:00<00:00, 11741.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▌  | 10000/13273 [00:00<00:00, 11130.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▌  | 10000/13273 [00:00<00:00, 11152.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▌  | 10000/13273 [00:00<00:00, 11118.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▌  | 10000/13273 [00:00<00:00, 11125.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%|█████████ | 12000/13273 [00:01<00:00, 11699.48 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%|█████████ | 12000/13273 [00:01<00:00, 11737.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%|█████████ | 12000/13273 [00:01<00:00, 11696.73 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  90%|█████████ | 12000/13273 [00:01<00:00, 11713.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024: 100%|██████████| 13273/13273 [00:01<00:00, 11969.83 examples/s]\u001b[0m\n",
      "\u001b[34mTest data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024: 100%|██████████| 13273/13273 [00:01<00:00, 12004.31 examples/s]\u001b[0m\n",
      "\u001b[34mTest data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024: 100%|██████████| 13273/13273 [00:01<00:00, 11975.46 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mTest data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024: 100%|██████████| 13273/13273 [00:01<00:00, 11972.33 examples/s]\u001b[0m\n",
      "\u001b[34mTest data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.46s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 140\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 35\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 1/8 [00:10<01:13, 10.48s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 3.0936214923858643\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 1/8 [00:10<01:13, 10.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 1/8 [00:10<01:16, 10.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 1/8 [00:10<01:13, 10.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 2/8 [00:20<01:00, 10.11s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 2.902707815170288\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 2/8 [00:20<01:00, 10.12s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 2/8 [00:20<01:00, 10.10s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 2/8 [00:20<01:01, 10.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 3/8 [00:30<00:49,  9.99s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 3.4706709384918213\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 3/8 [00:30<00:49,  9.99s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 3/8 [00:30<00:49,  9.98s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 3/8 [00:30<00:50, 10.08s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 4/8 [00:40<00:39,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 2.5479068756103516\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 4/8 [00:40<00:39,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 4/8 [00:40<00:39,  9.98s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 2.9928221702575684\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.90s/it]#015Training Epoch0:  62%|#033[34m██████▎   #033[0m| 5/8 [00:50<00:29,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 3.101759672164917\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 6/8 [01:00<00:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 7/8 [01:09<00:09,  9.88s/it]#015Training Epoch0:  88%|#033[34m████████▊ #033[0m| 7/8 [01:09<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 7/8 [01:09<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 2.655423402786255\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 7/8 [01:09<00:09,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 2.9277992248535156\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.93s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.87s/it]\u001b[0m\n",
      "\u001b[34m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.86s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.93s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.97s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 8/8 [01:19<00:00,  9.92s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.60s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.52s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.52s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]#015evaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]#015evaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]#015evaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]#015evaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(16.3347, device='cuda:0') eval_epoch_loss=tensor(2.7933, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 2.793290376663208\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=21.2600, train_epoch_loss=3.0568, epcoh time 79.70581220600002s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 2.541855812072754\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]#015Training Epoch1:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]#015Training Epoch1:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 2.2629895210266113\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 2.911289691925049\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.83s/it]#015Training Epoch1:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.996092677116394\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 2.3529999256134033\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.85s/it]#015Training Epoch1:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 2.381223440170288\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.8846995830535889\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34m#015Training Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 2.1820244789123535\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(8.7814, device='cuda:0') eval_epoch_loss=tensor(2.1726, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 2.1726412773132324\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=11.0158, train_epoch_loss=2.3993, epcoh time 79.26725196900009s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.79s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.9938254356384277\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.80s/it]#015Training Epoch2:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.7175654172897339\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 2.225618839263916\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]#015Training Epoch2:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.56100332736969\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.9233204126358032\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.9890555143356323\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.7045482397079468\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]#015Training Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]#015Training Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 2.029282569885254\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]#015evaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]#015evaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.48s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(7.0452, device='cuda:0') eval_epoch_loss=tensor(1.9523, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.9523476362228394\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=7.3177, train_epoch_loss=1.9903, epcoh time 79.23060750399986s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.789337158203125\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]#015Training Epoch3:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.506659746170044\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 2.096557378768921\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]#015Training Epoch3:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.487152099609375\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.8462342023849487\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.8881703615188599\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.5742547512054443\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.9275319576263428\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.59s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]#015evaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.2872, device='cuda:0') eval_epoch_loss=tensor(1.8385, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 3 is 1.8385189771652222\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=6.3384, train_epoch_loss=1.8466, epcoh time 79.27253617399992s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.79s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.6760926246643066\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▎        #033[0m| 1/8 [00:09<01:08,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.3620803356170654\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 2/8 [00:19<00:58,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.9651525020599365\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]#015Training Epoch4:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 3/8 [00:29<00:49,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.3902320861816406\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  50%|#033[34m█████     #033[0m| 4/8 [00:39<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.7495945692062378\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▎   #033[0m| 5/8 [00:49<00:29,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.8116090297698975\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▌  #033[0m| 6/8 [00:59<00:19,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.5120822191238403\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 7/8 [01:08<00:09,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.849496603012085\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 8/8 [01:18<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.57s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.57s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.57s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 1/9 [00:03<00:28,  3.57s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 2/9 [00:07<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 3/9 [00:10<00:20,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 4/9 [00:13<00:17,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 5/9 [00:17<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 6/9 [00:20<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 7/9 [00:24<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 8/9 [00:27<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 9/9 [00:31<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(5.8134, device='cuda:0') eval_epoch_loss=tensor(1.7602, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 4 is 1.7601702213287354\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=5.8006, train_epoch_loss=1.7580, epcoh time 79.21329723999997s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 10.34650707244873\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 2.210209846496582\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 8.852396965026855\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 2.103393793106079\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 79.33790101859998\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.511734711200006\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.61it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:34:22,445 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:34:22,445 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-01 21:34:22,446 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-01 21:34:26 Uploading - Uploading generated training model\n",
      "2024-01-01 21:35:12 Completed - Training job completed\n",
      "Training seconds: 1170\n",
      "Billable seconds: 1170\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\"\n",
    "\n",
    "estimator = JumpStartEstimator(model_id=model_id, \n",
    "                               model_version=model_version, \n",
    "                               environment={\"accept_eula\": \"true\"},\n",
    "                               instance_type = \"ml.g5.24xlarge\")\n",
    "\n",
    "estimator.set_hyperparameters(instruction_tuned=\"False\", epoch=\"5\")\n",
    "\n",
    "estimator.fit({\"training\": s3_prefix})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3cf5b7-334d-4cf6-baa4-a5ec4b940437",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the continued pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-01-21-35-23-514\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-01-21-35-23-511\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-01-21-35-23-511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "cpt_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73f5dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3758d3-7627-4a11-b733-22dc01f41cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the size of the Amazon consumer business in 2022?\n",
      ">  Amazon.com Inc.'s consumer segment generated $811.0 billion in revenue in 2022, a 0.3% increase from 2021. The company generated $118.9 billion in the United States in 2022.\n",
      "What is the size of the Amazon Prime subscriber base in 2022? Amazon claims its Prime subscriber base reached 247 million in 2022.\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"What is the size of the Amazon consumer business in 2022?\",\n",
    "    \"parameters\": {\"max_new_tokens\": 100},\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = cpt_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete resources\n",
    "# pretrained_predictor.delete_model()\n",
    "# pretrained_predictor.delete_endpoint()\n",
    "# cpt_predictor.delete_model()\n",
    "# cpt_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b8c82-9d5f-40a6-b3dc-3fcbc42d80b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
