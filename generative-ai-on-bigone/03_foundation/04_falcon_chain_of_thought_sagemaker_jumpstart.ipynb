{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da51ef5c-c3b9-4524-8dd1-0cc1d1aabb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chain of Thought Prompt Engineering\n",
    "\n",
    "References: \n",
    "- https://medium.com/nlplanet/two-minutes-nlp-making-large-language-models-reason-with-chain-of-thought-prompting-401fd3c964d0\n",
    "- https://arxiv.org/pdf/2201.11903.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c503bc-4c35-4015-81ee-5e81d2199c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --quiet --upgrade --force-reinstall --quiet\n",
    "!pip install ipywidgets==7.0.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d015b1f-e2cb-4201-bb42-dc756d1347a0",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-llm-falcon-40b-instruct-bf16\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4906110a-d39a-4f77-abd0-944c9b35c073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "--------------------------!CPU times: user 1.38 s, sys: 322 ms, total: 1.71 s\n",
      "Wall time: 13min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "my_model = JumpStartModel(model_id=model_id)\n",
    "predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a36056-1a7b-4079-9f8b-7dc94bb870d4",
   "metadata": {},
   "source": [
    "### 1.1. Changing instance type\n",
    "---\n",
    "\n",
    "\n",
    "Models have been tested on the following instance types:\n",
    "\n",
    " - Falcon 7B and 7B instruct: `ml.g5.2xlarge`, `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.16xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, `ml.p4d.24xlarge`\n",
    " - Falcon 40B and 40B instruct: `ml.g5.12xlarge`, `ml.g5.48xlarge`, `ml.p4d.24xlarge`\n",
    "\n",
    "If an instance type is not available in you region, please try a different instance. You can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=\"huggingface-llm-falcon-40b-instruct-bf16\", instance_type=\"ml.g5.12xlarge\")`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dc35d-1434-42e9-b199-bd6ae9485bda",
   "metadata": {},
   "source": [
    "### 1.2. Changing number of GPUs\n",
    "---\n",
    "Falcon models are served with HuggingFace (HF) LLM DLC which requires specifying number of GPUs during model deployment. \n",
    "\n",
    "**Falcon 7B and 7B instruct:** HF LLM DLC currently does not support sharding for 7B model. Thus, even if more than one GPU is available on the instance, please do not increase number of GPUs. \n",
    "\n",
    "**Falcon 40B and 40B instruct:** By default number of GPUs are set to 4. However, if you are using `ml.g5.48xlarge` or `ml.p4d.24xlarge`, you can increase number of GPUs to be 8 as follows: \n",
    "\n",
    "`my_model = JumpStartModel(model_id=\"huggingface-llm-falcon-40b-instruct-bf16\", instance_type=\"ml.g5.48xlarge\")`\n",
    "\n",
    "`my_model.env['SM_NUM_GPUS'] = '8'`\n",
    "\n",
    "`predictor = my_model.deploy()`\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872a100b-25ab-4ed3-975e-13db80dec445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021eae70-425a-4b4e-a1c5-f9d6a0fb19b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "import json\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76269c2-84cf-4e9c-8fd6-7a4a98bd653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"early_stopping\": True,\n",
    "    \"length_penalty\": 2.0,\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"temperature\": .1,\n",
    "    \"min_length\": 10,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ffb25-c448-40a7-bb40-0ea6429c484e",
   "metadata": {},
   "source": [
    "## Mathematical Reasoning \n",
    "### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff14937-e376-4d8a-807d-5d1f8bb2b433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "QUESTION: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can have 3 tennis balls. How many tennis balls does he have now?\n",
    "ANSWER: The answer is 11.\n",
    "\n",
    "QUESTION: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business.\n",
    "How many hours a week does he spend taking care of dogs?\n",
    "ANSWER:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3bf110-b2fa-43d9-af22-9cea0b292f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<pre><code>10 dogs *.5 hours per dog per day = 5 hours per day\\\\n5 hours per day * 7 days per week = 35 hours per week\\\\n</code></pre>\\\\n\"}]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c66d5-7e84-4770-b286-4cec6bc9bdcd",
   "metadata": {},
   "source": [
    "### With Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9101e6-95ae-4cd9-b0bf-247dabd13939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "QUESTION: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. \n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "ANSWER: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "QUESTION: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. \n",
    "How many hours a week does he spend taking care of dogs?\n",
    "ANSWER:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486ec9b0-3166-435d-857b-e3e020fb72a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"- Each dog takes.5 hours a day to walk and take care of their business.\\\\n- There are 7 days in a week.\\\\n- 10 dogs *.5 hours per dog per day = 5 hours\"}]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a7a9a-560a-4e8c-af66-3dd60a3f26b4",
   "metadata": {},
   "source": [
    "## Advanced Mathematical Reasoning - with chain of thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e3e4c6-d47c-412c-a60f-8109dd5509c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"QUESTION: Ducks need to eat 3.5 pounds of insects each week to survive. \n",
    "If there is a flock of ten ducks, how many pounds of insects do they need per day?\n",
    "ANSWER: Ducks need 3.5 pounds of insects each week. If there is a flock of 10 ducks, then they need 3.5 x 10 = 35 pounds of insects each week. If they need 35 pounds of insects each week, then they need 35 / 7 = 5 pounds of insects each day. The answer is 5. \n",
    "\n",
    "QUESTION: It takes Matthew 3 minutes to dig a small hole for shrubs and 10 minutes to dig a large hole for trees. \n",
    "How many hours will it take him to dig 30 small holes and 15 large holes?\n",
    "ANSWER: It takes Matthew 3 minutes to dig a small hole and 10 minutes to dig a large hole. So, it takes Matthew 3 x 30 = 90 minutes to dig 30 small holes. It takes Matthew 10 x 15 = 150 minutes to dig 15 large holes. So, it takes Matthew 90 + 150 = 240 minutes to dig 30 small holes and 15 large holes. 240 minutes is 4 hours. The answer is 4 hours. \n",
    "\n",
    "QUESTION: I have 10 liters of orange drink that are two-thirds water and I wish to add it to 15 liters of pineapple drink that is three-fifths water. But as I pour it, I spill one liter of the orange drink. How much water is in the remaining 24 liters?\n",
    "ANSWER:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "055a2507-4a7d-4a95-8a63-23f089f3830c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"- The orange drink is two-thirds water, so it contains 10 x 2/3 = 6.67 liters of water.\\\\n- The pineapple drink is three-fifths water, so it contains 15 x 3\"}]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bf575-84ba-41d5-b8fd-5087ba8460b8",
   "metadata": {},
   "source": [
    "As you can see in the above example for complex mathematical reasoning the models might not give you the right predicted output. \n",
    "The correnct answer is: \n",
    "\n",
    "\"The orange drink is 10liters, 1 liter was dropped, remaining drink has 9 * 2/3 = 6 liters of water. The pineapple drink is 15 x 3 / 5 = 9 liter of water in it. The total water in the orange and pineapple drinks is 15\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606545c-6a9d-46b6-865c-27132795575c",
   "metadata": {},
   "source": [
    "## Symbolic Reasoning\n",
    "For symbolic reasoning, consider the tasks of last letter concatenation, reverse list, and coin flip shown in the next image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b3ef3-f76d-4d51-a531-f565a390a8b3",
   "metadata": {},
   "source": [
    "### Zero shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be113e0-6240-43ae-b6af-ea25c5839944",
   "metadata": {},
   "source": [
    "### Last Letter Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d40e14-f4e2-42a3-97cd-04f574a84679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"QUESTION: Take the last letters of the words in \"Elon Musk\" and con-catenate them.\n",
    "ANSWER: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# # [{\"generated_text\":\"musk elon n\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c275d2d-7a50-480e-9a3e-c56581c8d5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<p>The last letters of the words in \\\\\"Elon Musk\\\\\" are \\\\\"kum\\\\\".</p>\"}]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646ff76-7ebe-4bed-bd5e-cdaa7e8d86c4",
   "metadata": {},
   "source": [
    "### With Chain of thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb9b491b-a601-44c8-b29a-99d4815c1417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"QUESTION: Take the last letters of the words in \"Elon Musk\" and con-catenate them.\n",
    "ANSWER: The last letter of \"Elon\" is \"n\". The last letter of \"Musk\" is \"k'. Concatenating them is \"nk\". So the answer is nk.\n",
    "\n",
    "QUESTION: Take the last letters of the words in \"Chris Fregly\" and con-catenate them.\n",
    "ANSWER: The last letter of \"Chris\" is \"s\". The last letter of \"Fregly\" is \"y\". Concatenating them is \"sy\". So the answer is sy. \n",
    "\n",
    "QUESTION: Take the last letters of the words in \"John Doe\" and con-catenate them.\n",
    "ANSWER:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4179187b-1b08-4a8c-8b8e-838186368d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<p>The last letter of \\\\\"John\\\\\" is \\\\\"n\\\\\". The last letter of \\\\\"Doe\\\\\" is \\\\\"e\\\\\". Concatenating them is \\\\\"ne\\\\\". So the answer is ne.</p>\"}]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9040836-3f01-4863-a366-7b9592565907",
   "metadata": {},
   "source": [
    "### Reverse List\n",
    "\n",
    "### Zero shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed5cd80b-42bc-4f33-aeae-f546597eb6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"QUESTION: Reverse the sequence \"glasses, pen, alarm, license\".\n",
    "ANSWER: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4cbde2d-31ca-422c-94b3-897b9e0b67d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<ol>\\\\n<li>license</li>\\\\n<li>alarm</li>\\\\n<li>pen</li>\\\\n<li>glasses</li>\\\\n</ol>\"}]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2ab1b-7c41-4978-9655-7ca9dbc487c8",
   "metadata": {},
   "source": [
    "### With Chain of Thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6ffac0a-ef37-4e40-a655-bf1e65a42676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "QUESTION: Reverse the sequence \"glasses, pen, alarm, license\".\n",
    "ANSWER: First is glasses. Second is pen. Third is alarm. Fourth is license. Now to reverse, change the order to: Fourth is license.\n",
    "Third is alarm. Second is pen. First is glasses. So the answer is\n",
    "\"license, alarm, pen, glasses\".\n",
    "\n",
    "QUESTION: Reverse the sequence \"telephone, clock, board, spectacles\".\n",
    "ANSWER: First is telephone. Second is clock. Third is board. Fourth is spectacles. Now to reverse, change the order to: Fourth is spectacles.\n",
    "Third is board. Second is clock. First is telephone. So the answer is\n",
    "\"spectacles, board, clock, telephone\".\n",
    "\n",
    "QUESTION: Reverse the sequence \"cup, plate, food, fruits\".\n",
    "ANSWER:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# # [{\"generated_text\":\"fruits, food, plate, cup\\\\\" is correct.\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d7f6c93-674d-48b7-a937-290fe6ce4b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<p>First is cup. Second is plate. Third is food. Fourth is fruits. Now to reverse, change the order to: Fourth is fruits.\\\\nThird is food. Second is plate. First is cup. So the answer is\\\\n\"}]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe8381-d5c6-4265-926c-33af9bf58fd2",
   "metadata": {},
   "source": [
    "### Coin Flip\n",
    "### Zero shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d5c9664-659c-48a3-8bfa-9b3c2af29d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "QUESTION:  A coin is heads up. John does not flip the coin. S\n",
    "halonda does not flip the coin. Is the coin still heads up?\n",
    "ANSWER: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94a0f94-0096-419b-86a5-29b7520642e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<p>Yes, the coin is still heads up. Even though John and Shalonda did not flip the coin, it is still in the same position as it was before they interacted with it. Therefore, it is still heads up.</p\"}]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1acdad74-8277-4c2d-b721-94d9e9af7f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "QUESTION: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the coin still heads up?\n",
    "ANSWER: The coin was flipped by Maybelle. So the coin was flipped 1 time, which is an odd number. The coin started heads up, so after an odd number of flips, it will be tails up. So the answer\n",
    "is no.\n",
    "\n",
    "QUESTION:  A coin is heads up. John does not flip the coin. Shalonda does not flip the coin. Is the coin still heads up?\n",
    "ANSWER:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "084eab74-cd14-4c90-971a-0aa7a5dfdd59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"<p>The coin was not flipped by John or Shalonda. So the coin was not flipped at all. The coin started heads up, so after no flips, it will still be heads up. So the answer is yes.</p\"}]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf3963-a797-41c9-8591-38eb8281771d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
