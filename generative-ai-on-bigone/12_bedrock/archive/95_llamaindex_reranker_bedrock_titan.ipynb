{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b23ffc-22d8-4414-b2e4-66d45a03523d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using LLM-powered retreival and reranking - (Titan LLM + Bedrock Titan embedding)\n",
    "\n",
    "### Context\n",
    "\n",
    "Utilizing LLM-driven retrieval has the potential to yield more pertinent documents compared to retrieval based on embeddings. However, this advantage comes at the expense of increased latency and expenses. We will demonstrate that employing embedding-based retrieval initially, followed by a secondary retrieval stage for reevaluation, can offer a balanced solution.\n",
    "\n",
    "A recent surge in applications involving \"Develop a chatbot using your data\" has emerged in the past several months. This trend has been facilitated by frameworks such as LlamaIndex and LangChain. Many of these applications rely on a standard approach known as retrieval augmented generation (RAG):\n",
    "\n",
    "1) A vector store is employed to store unstructured documents (knowledge corpus).\n",
    "2) When presented with a query, a retrieval model is utilized to fetch relevant documents from the corpus, followed by a synthesis model that generates a response.\n",
    "3) The retrieval model retrieves the top-k documents based on the similarity of their embeddings to the query. It's important to note that the concept of top-k embedding-based semantic search has existed for over a decade and doesn't involve the use of LLM.\n",
    "\n",
    "The utilization of embedding-based retrieval offers numerous advantages:\n",
    "\n",
    "* Dot product calculations are swift and don't necessitate model invocations during query processing.\n",
    "* Although not flawless, embeddings can effectively capture the semantics of documents and queries. There's a subset of queries for which embedding-based retrieval yields highly relevant outcomes.\n",
    "\n",
    "However, embedding-based retrieval can exhibit imprecision and return irrelevant context for the query due to various factors. This subsequently diminishes the quality of the overall RAG system, irrespective of the LLM's quality.\n",
    "\n",
    "Addressing this challenge is not novel; existing information retrieval and recommendation systems have adopted a two-stage approach. The initial stage employs embedding-based retrieval with a higher top-k value to maximize recall while accepting a lower precision. Subsequently, the second stage utilizes a somewhat more computationally intensive process characterized by higher precision and lower recall (such as BM25) to \"rerank\" the initially retrieved candidates.\n",
    "\n",
    "Delving into the shortcomings of embedding-based retrieval would require an entire series of blog posts. This current post serves as an initial exploration of an alternative retrieval technique and its potential to enhance embedding-based retrieval methodologies.\n",
    " \n",
    "![LLM retrival works](./images/arch.png)\n",
    "\n",
    "### LLM Retrieval and Reranking\n",
    "\n",
    "LLM Retrieval and reranking strategy employs the LLM to determine the document(s) or sections of text that align with the provided query. The input prompt comprises a collection of potential documents, and the LLM is entrusted with choosing the pertinent group of documents while also assigning a score to gauge their relevance using an internal measurement.\n",
    "\n",
    "\n",
    "In this notebook we explain how to approach the retriever pattern of LLM-powered retrieval and reranking using Amazon Bedrock LLM and LlamaIndex\n",
    "\n",
    "#### LlamaIndex\n",
    "LlamaIndex is a data framework for your LLM application. It provides the following tools:\n",
    "\n",
    "* Offers data connectors to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "* Provides ways to structure your data (indices, graphs) so that this data can be easily used with LLMs.\n",
    "* Provides an advanced retrieval/query interface over your data: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "* Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, anything else).\n",
    "* LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "### LLM Used:\n",
    "We will be leveraging Bedrock - Anthropic Titan LLM and Bedrock Embedding (Titan model) for demonstration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320367c-42b6-46e3-b367-de7327d6e665",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will first install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074cdec9-58a4-4894-b90e-53170777c673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd .. && ./download-dependencies.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44150464-2078-4c67-8dc2-e485df8f55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "botocore_whl_filename = glob.glob(\"../dependencies/botocore-*-py3-none-any.whl\")[0]\n",
    "boto3_whl_filename = glob.glob(\"../dependencies/boto3-*-py3-none-any.whl\")[0]\n",
    "\n",
    "subprocess.Popen(['pip', 'install', botocore_whl_filename, boto3_whl_filename, '--force-reinstall'], bufsize=1, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c0cec-b59c-4c69-9e79-6979d0e758ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pydantic==1.10.12 --force-reinstall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e83f2-1ab2-4fc3-a120-5d084fe7f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain==0.0.266 is required by llama-index==0.8.8\n",
    "%pip install langchain==0.0.266 \\\n",
    "    pypdf==3.15.2 \\\n",
    "    llama-index==0.8.8 \\\n",
    "    sentence_transformers==2.2.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68a3ee-f4c1-4830-856a-9c37f7a42364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16404b-db81-4ee6-9c0f-cc4cdf3cb147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    "    get_response_synthesizer,\n",
    "    set_global_service_context,\n",
    "    StorageContext,\n",
    "    ListIndex\n",
    ")\n",
    "from llama_index.indices.postprocessor import LLMRerank\n",
    "from llama_index.llms import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8809c-b174-4970-8758-f867cf968d81",
   "metadata": {},
   "source": [
    "### Setup langchain and llama index\n",
    "\n",
    "In this step we will be creating of instance for LLM and embedding models. We will be using Claude and Titan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0514e54-36cd-4faf-b824-4cce34177bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import LangchainEmbedding\n",
    "from langchain.llms.bedrock import Bedrock \n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "model_kwargs_titan = { \n",
    "        \"maxTokenCount\": 512,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\":0.0,  \n",
    "        \"topP\":0.5\n",
    "    }\n",
    "\n",
    "llm = Bedrock(model_id=\"amazon.titan-tg1-large\",\n",
    "              model_kwargs=model_kwargs_titan)\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "    BedrockEmbeddings(model_id=\"amazon.titan-e1t-medium\")\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embed_model, \n",
    "                                               chunk_size=512)\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2f25b-2d06-42c7-8d65-9fd2c063ed81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491600b-32fa-4559-8d25-c7d07a57b84b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2022, source=filenames[0]),\n",
    "    dict(year=2021, source=filenames[1]),\n",
    "    dict(year=2020, source=filenames[2]),\n",
    "    dict(year=2019, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61931154-fabf-41f2-8c1a-f7db4df8d304",
   "metadata": {},
   "source": [
    "As part of Amazon's culture, the CEO always includes a copy of the 1997 Letter to Shareholders with every new release. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 1997 letter (last 3 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0b300-0c6c-43be-a2ea-ae16fa0912f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "for local_pdf in local_pdfs:\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    for pagenum in range(len(pdf_reader.pages)-3):\n",
    "        page = pdf_reader.pages[pagenum]\n",
    "        pdf_writer.add_page(page)\n",
    "\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0feafd-12ef-40b4-9bb4-99ff7bc8bac9",
   "metadata": {},
   "source": [
    "Now that you have clean PDFs to work with, you will enrich your documents with metadata, then use a process called \"chunking\" to break up a larger document into small pieces. These small pieces will allow you to generate embeddings without surpassing the input limit of the embedding model.\n",
    "\n",
    "In this example you will break the document into 1000 character chunks, with a 100 character overlap. This will allow your embeddings to maintain some of its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429b839-e244-42b8-9b24-5368c1bb0a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for filename in filenames:\n",
    "    doc = SimpleDirectoryReader(input_files=[f\"data/{filename}\"]).load_data()\n",
    "    doc[0].doc_id = filename.replace(\".pdf\", \"\")\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41704f08-d210-4c7e-aa03-f3f8c48252e8",
   "metadata": {},
   "source": [
    "### Build Document Summary Index\n",
    "\n",
    "We show two ways of building the index:\n",
    "- default mode of building the document summary index\n",
    "- customizing the summary query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3004bc-c525-4b57-aebd-b0d38b3a8cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Un comment the following lines to run from your local environment outside of the AWS account with Bedrock access\n",
    "\n",
    "#import os\n",
    "#os.environ['BEDROCK_ASSUME_ROLE'] = '<YOUR_VALUES>'\n",
    "#os.environ['AWS_PROFILE'] = 'bedrock-user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ea0a6-7563-4005-a573-10ce0ebbc0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'\n",
    "boto3_bedrock = bedrock.get_bedrock_client(os.environ.get('BEDROCK_ASSUME_ROLE', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c912e31-a44b-4a65-b75f-212413865acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8555b6d-5289-4803-bced-6b2f85dea3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(docs,\n",
    "    service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba68d6-3a91-4b31-a8b9-b7cc6d33b5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = service_context.node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413a570-686c-4865-a424-7f69b822b640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805847d9-a2c1-4930-98a9-98126e730000",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace86835-832e-4964-a025-428891aa2c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k\n",
    "\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if with_reranker:\n",
    "        # configure reranker\n",
    "        reranker = LLMRerank(\n",
    "            choice_batch_size=5, \n",
    "            top_n=reranker_top_n, \n",
    "            service_context=service_context\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57492945-9ac6-44d9-9588-d6f97a460b0c",
   "metadata": {},
   "source": [
    "Now, we will showcase how to do a two-stage pass for retrieval. Use embedding-based retrieval with a high top-k value in order to maximize recall and get a large set of candidate items. Then, use LLM-based retrieval to dynamically select the nodes that are actually relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20993200-b725-403e-8a79-e2571dab2ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes1 = get_retrieved_nodes(\n",
    "    \"How has AWS evolved?\", vector_top_k=3, with_reranker=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65220b0-ce3d-4bfb-a409-35d7cc035a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_nodes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3bc25-6ef1-46b7-869e-247c83af9d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, node in enumerate(retrieved_nodes1):\n",
    "    print(node.score)\n",
    "    print(node.node.get_text())\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7d62b-dba9-45d5-896f-4baa9a40edba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes1_withreranker = get_retrieved_nodes(\n",
    "    \"How has AWS evolved?\",\n",
    "    vector_top_k=3,\n",
    "    reranker_top_n=1,\n",
    "    with_reranker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d031c16-16ac-425e-ad56-3b9ef0dd0fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_nodes1_withreranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22d297-dd3b-4a71-a890-faceafab4e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, node in enumerate(retrieved_nodes1_withreranker):\n",
    "    print(node.score)\n",
    "    print(node.node.get_text())\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033d487-3b31-4c95-9a63-afec75e3bc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes2 = get_retrieved_nodes(\n",
    "    \"Human: Why is Amazon successful?\", vector_top_k=3, with_reranker=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a3658-740c-4efa-b73d-d102528ddf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104633ac-d7ee-496d-8c4b-44aa462098f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, node in enumerate(retrieved_nodes2):\n",
    "    print(node.score)\n",
    "    print(node.node.get_text())\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0acde8-cb0a-4fa8-b30f-4fc965d8e7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes2_withreranker = get_retrieved_nodes(\n",
    "    \"Human: Why is Amazon successful?\",\n",
    "    vector_top_k=3,\n",
    "    reranker_top_n=1,\n",
    "    with_reranker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba718e-4ae9-4e15-8739-4f68f2d046b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_nodes2_withreranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04678820-d492-4c4a-b246-04a10e7d6b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, node in enumerate(retrieved_nodes2_withreranker):\n",
    "    print(node.score)\n",
    "    print(node.node.get_text())\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f0d82-a9e4-4736-824c-3223b21247c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
